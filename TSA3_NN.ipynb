{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdbb88b-cd17-44a4-855b-5a394df1df49",
   "metadata": {},
   "source": [
    "![Twitter Sentiment Analysis](tt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95957a9b-7e12-4ec0-862a-10ea942c82e1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis From Scratch: A Deep Dive into Twitter Data\n",
    "\n",
    "### Notebook 3: Neural Network Modeling\n",
    "\n",
    "In this notebook, the focus is on the construction, training, and evaluation of neural networks for sentiment classification. Vectorization methods trained in the previous notebook are used to prepare the data. Various neural network architectures and hyperparameters are systematically tested to identify the most effective combination for sentiment analysis. To ensure transparency and reproducibility, MLflow is employed as a central tracking tool, capturing details, metrics, and outcomes of each model iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c442c3-bc9a-4e09-8bdf-36218988a6fe",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Importing Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05a7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries for file and time operations\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Library for regular expressions\n",
    "import re\n",
    "\n",
    "# Libraries for random number generation and suppressing warnings\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Libraries for numerical operations and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Library for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "# Libraries for model evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Library for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for text tokenization\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Library for Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Libraries for deep learning with PyTorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import GRU\n",
    "\n",
    "# Libraries for plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Library for clearing Jupyter Notebook cell output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Libraries for statistical tests\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu,kruskal,shapiro,levene, bartlett\n",
    "\n",
    "# Inline plotting for Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba450d5-827f-4bb3-b17b-1c0476cccd23",
   "metadata": {},
   "source": [
    "### Setting seeds for reproducibility\n",
    "\n",
    "In the field of data science and machine learning, it is paramount to ensure the reproducibility of experiments. This is achieved by setting a seed value for random number generation, which helps to maintain the consistency of the results across multiple runs. This procedure establishes deterministic behavior for operations that would otherwise be random, facilitating the precise replication of results by other researchers or by themselves at a later date.\r\n",
    "\r\n",
    "However, it is vital to note that, despite having set seed values to ensure reproducibility, some functions and operations within PyTorch may still operate in a non-deterministic manner. This means that, even when recreating the environment and the conditions of the experiment, small variations in the results can be observed with each new run. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04005225-aa87-4ea8-b94d-f2d7e1f30602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfeaba-1dd4-4ee2-a99e-d5496a821aef",
   "metadata": {},
   "source": [
    "### Loading Dataset cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e96725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "      <th>text3</th>\n",
       "      <th>text4</th>\n",
       "      <th>text5</th>\n",
       "      <th>text6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>@switchfoot - awww, that's bummer. shoulda got...</td>\n",
       "      <td>switchfoot awww bummer shoulda got david carr ...</td>\n",
       "      <td>switchfoot awww bummer shoulda got david carr ...</td>\n",
       "      <td>switchfoot awww bummer shoulda got david carr ...</td>\n",
       "      <td>switchfoot awww bummer shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset can't update facebook texting it... migh...</td>\n",
       "      <td>upset update facebook texting ... might cry re...</td>\n",
       "      <td>upset update facebook texting ... might cry re...</td>\n",
       "      <td>upset updat facebook text ... might cri result...</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@kenichan dived many times ball. managed save ...</td>\n",
       "      <td>kenichan dived many times ball managed save 50...</td>\n",
       "      <td>kenichan dived many times ball managed save 50...</td>\n",
       "      <td>kenichan dive mani time ball manag save 50 res...</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       0  is upset that he can't update his Facebook by ...   \n",
       "2       0  @Kenichan I dived many times for the ball. Man...   \n",
       "\n",
       "                                               text2  \\\n",
       "0  @switchfoot - awww, that's bummer. shoulda got...   \n",
       "1  upset can't update facebook texting it... migh...   \n",
       "2  @kenichan dived many times ball. managed save ...   \n",
       "\n",
       "                                               text3  \\\n",
       "0  switchfoot awww bummer shoulda got david carr ...   \n",
       "1  upset update facebook texting ... might cry re...   \n",
       "2  kenichan dived many times ball managed save 50...   \n",
       "\n",
       "                                               text4  \\\n",
       "0  switchfoot awww bummer shoulda got david carr ...   \n",
       "1  upset update facebook texting ... might cry re...   \n",
       "2  kenichan dived many times ball managed save 50...   \n",
       "\n",
       "                                               text5  \\\n",
       "0  switchfoot awww bummer shoulda got david carr ...   \n",
       "1  upset updat facebook text ... might cri result...   \n",
       "2  kenichan dive mani time ball manag save 50 res...   \n",
       "\n",
       "                                               text6  \n",
       "0  switchfoot awww bummer shoulda got david carr ...  \n",
       "1  upset updat facebook text might cri result sch...  \n",
       "2  kenichan dive mani time ball manag save rest g...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('training.1600000.processed.noemoticon_cleaned.csv')\n",
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b062342-c0a4-415a-8878-28406425f11d",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Testing Sets\n",
    "\n",
    "To robustly evaluate model performance, the data is split into training and testing sets. The training set is used to train the model, while the testing set is reserved to assess the model's performance on previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets['text6'], tweets['target'], random_state = 5, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9cbae-3b1e-41ba-a772-0ecf73c09767",
   "metadata": {},
   "source": [
    "### Data Vectorization\n",
    "\n",
    "In this step, the previously trained Word2Vec models (CBOW and Skip-Gram) are used to convert the tweet text into numerical vectors. This vectorization is essential for preparing the data for neural network training. Each tweet is transformed into a dense numerical representation, which serves as input for the sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a8ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the pre-trained Word2Vec models (CBOW and SG)\n",
    "cbow_path = 'models/word2vec_cbow.model'\n",
    "sg_path = 'models/word2vec_sg.model'\n",
    "\n",
    "# Load the pre-trained models from the specified paths\n",
    "cbow = Word2Vec.load(cbow_path)\n",
    "sg = Word2Vec.load(sg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff9a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the whitespace tokenizer from the nltk library\n",
    "ws_tokenizer = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# Defines a function to convert text into numerical vectors using a given Word2Vec model\n",
    "def vectorize (text, modelo):\n",
    "    # Gets the number of text samples and sets the vector dimension (300 in this case)\n",
    "    x = len(text)\n",
    "    y = 300\n",
    "\n",
    "    # Initializes a zero matrix of shape (x, y)\n",
    "    matrix = np.zeros((x,y))\n",
    "\n",
    "    # Iterates through each text sample in the input DataFrame\n",
    "    for i in range(x):\n",
    "        # Tokenizes the text sample into words using whitespace as the delimiter\n",
    "        words = ws_tokenizer.tokenize(text.iloc[i])\n",
    "\n",
    "        # Iterates through each word in the tokenized text\n",
    "        for word in words:\n",
    "            # Checks if the word exists in the Word2Vec model's vocabulary\n",
    "            if word in modelo.wv:\n",
    "                # Adds the word vector to the corresponding row in the matrix\n",
    "                matrix[i] += modelo.wv.get_vector(word)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275fb30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorized. Elapsed Time: 61.51 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Records the time at which the vectorization process starts\n",
    "start_time = time.time()\n",
    "\n",
    "# Vectorizes training and test data using CBOW model\n",
    "matrix_train_cbow = vectorize(X_train, cbow)\n",
    "matrix_test_cbow = vectorize(X_test, cbow)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print('Data Vectorized. Elapsed Time: {:.2f} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3623bf5d-2536-4a1f-92b8-9ef1670abf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorized. Elapsed Time: 61.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Records the time at which the vectorization process starts\n",
    "start_time = time.time()\n",
    "\n",
    "# Vectorizes training and test data using SG model\n",
    "matrix_train_sg= vectorize(X_train, sg)\n",
    "matrix_test_sg = vectorize(X_test, sg)\n",
    "\n",
    "# End timer and print elapsed time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print('Data Vectorized. Elapsed Time: {:.2f} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c5fc1-11f0-4e43-a30c-1afed1092a4c",
   "metadata": {},
   "source": [
    "### Setting Up the Computational Device\n",
    "In deep learning, especially when dealing with large datasets, utilizing a GPU can significantly accelerate the training process. Before proceeding with training the neural network, it is checked whether a GPU is available. If so, the device is set to use the GPU; otherwise, it defaults to the CPU. The selected device will be used in subsequent computations and model training. By displaying the chosen device, one can confirm the computational resource that will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c043d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Choose GPU if available, otherwise use CPU and print the device being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # Use CPU\n",
    "\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e226ab-f6a9-4a74-bacd-58f2a6f59d65",
   "metadata": {},
   "source": [
    "## First Experiment: A basic neural network\n",
    "\n",
    "In the initial experiment, the focus is on training a basic neural network to serve as a foundational model for sentiment analysis. This approach provides a benchmark for more complex architectures and helps to establish a baseline understanding of the dataset's characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6352a8-009b-48a0-a1b5-354689ed376f",
   "metadata": {},
   "source": [
    "### Setting up the Neural Network Architecture\n",
    "\n",
    "In this section, the neural network architecture is defined, and functions for training and testing the model are implemented. The `classifier` class sets up the network layers, while the `train` and `test` functions manage the training and evaluation process. Additionally, `MLflow` is utilized to facilitate the tracking and analysis of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d45163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network classifier\n",
    "class classifier (nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "        super(classifier, self).__init__()\n",
    "\n",
    "        # Define hidden layer with ReLU activation\n",
    "        self.features = nn.Sequential(nn.Linear(input_size,hidden_size),\n",
    "                                      nn.ReLU()\n",
    "                                     )\n",
    "        # Define output layer\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    # Define forward pass\n",
    "    def forward(self, X):\n",
    "        feature = self.features(X)\n",
    "        output = self.out(feature)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1291512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for a single epoch\n",
    "def train(train_loader, net, epoch, criterion ,optimizer):\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # Initialize list to store batch losses\n",
    "    epoch_loss = []\n",
    "\n",
    "    # Iterate through each batch in the training data loader\n",
    "    for batch in train_loader:\n",
    "        dado,rotulo = batch\n",
    "        \n",
    "        # Move data and labels to the chosen device (CPU or GPU)\n",
    "        dado,rotulo = dado.to(device), rotulo.to(device)\n",
    "        \n",
    "        # Forward pass: compute predictions and loss\n",
    "        pred = net(dado)\n",
    "        loss = criterion (pred.squeeze(), rotulo.float())\n",
    "        epoch_loss.append(loss.cpu().data)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert epoch loss to NumPy array and compute mean\n",
    "    epoch_loss = np.asarray(epoch_loss)\n",
    "    \n",
    "    return epoch_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66c6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function for a single epoch\n",
    "def test(test_loader, net, epoch, criterion):\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient calculations for performance\n",
    "    with torch.no_grad():\n",
    "        # Initialize variable for loss\n",
    "        epoch_loss = []\n",
    "        \n",
    "        # Iterate through each batch in the test data loader\n",
    "        for batch in test_loader:\n",
    "            dado,rotulo = batch\n",
    "\n",
    "            # Move data and labels to the chosen device (CPU or GPU)\n",
    "            dado,rotulo = dado.to(device), rotulo.to(device)\n",
    "\n",
    "             # Forward pass: compute predictions and loss\n",
    "            pred = net(dado)\n",
    "            \n",
    "            loss = criterion(pred.squeeze(), rotulo.float())\n",
    "            epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "        # Convert epoch loss to NumPy array and compute mean\n",
    "        epoch_loss = np.asarray(epoch_loss)\n",
    "        \n",
    "        return epoch_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3c50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to train the neural network\n",
    "def train_nn(matrix_train, matrix_test, args, hidden_size = 32, input_size = 300, output_size = 1):\n",
    "    # Start MLflow run for experiment tracking\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        # Log hyperparameters\n",
    "        for key, value in args.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Log neural network architecture details\n",
    "        mlflow.log_param('input_size', input_size)\n",
    "        mlflow.log_param('hidden_size', hidden_size)\n",
    "        mlflow.log_param('output_size', output_size)\n",
    "\n",
    "        # Converts training and testing matrices to PyTorch tensors\n",
    "        X_train = torch.from_numpy(matrix_train).float()\n",
    "        Y_train = torch.from_numpy(y_train.values)\n",
    "        train_data = TensorDataset(X_train, Y_train)\n",
    "\n",
    "        X_test = torch.from_numpy(matrix_test).float()\n",
    "        Y_test = torch.from_numpy(y_test.values)\n",
    "        test_data = TensorDataset(X_test, Y_test)\n",
    "\n",
    "        # Creates data loaders for training and testing datasets\n",
    "        train_loader = DataLoader(train_data,\n",
    "                             batch_size=args['batch_size'],\n",
    "                             shuffle=True,\n",
    "                             num_workers=args['num_workers'])\n",
    "\n",
    "        test_loader = DataLoader(test_data,\n",
    "                                 batch_size=args['batch_size'],\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=args['num_workers'])\n",
    "\n",
    "        # Initializes the neural network classifier\n",
    "        net = classifier(input_size,hidden_size,output_size)\n",
    "        net.to(device)\n",
    "\n",
    "        # Defines the loss function\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        \n",
    "        # Chooses the optimizer based on user input\n",
    "        if args['optimizer'] == 'SGD':\n",
    "            optimizer = optim.SGD(net.parameters(), lr=args['lr'])\n",
    "        \n",
    "        if args['optimizer'] == 'Adam':\n",
    "            optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "        # Initializes variables to keep track of losses and other metrics\n",
    "        train_losses, test_losses = [], []\n",
    "        best_test_loss = float('inf')\n",
    "        best_model_epoch = -1\n",
    "        no_improvement_epochs = 0\n",
    "\n",
    "        # Records the time at which training starts\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(args['num_epochs']):\n",
    "            # Clears the output and prints the current training status\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print('Training neural network... Epoch ' + str(epoch) + '/' + str(args['num_epochs']-1))\n",
    "            if best_model_epoch!=-1:\n",
    "                print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "\n",
    "            # Trains the network and logs the loss\n",
    "            epoch_train_loss = train(train_loader, net, epoch,criterion, optimizer)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            mlflow.log_metric(\"epoch_train_loss\", epoch_train_loss, step=epoch)\n",
    "\n",
    "            # Tests the network and logs the loss\n",
    "            epoch_test_loss = test(test_loader, net, epoch, criterion)\n",
    "            test_losses.append(epoch_test_loss)\n",
    "            mlflow.log_metric(\"epoch_test_loss\", epoch_test_loss, step=epoch)\n",
    "\n",
    "            # Checks for improvements in test loss\n",
    "            dif = best_test_loss - epoch_test_loss\n",
    "            if epoch_test_loss < best_test_loss:\n",
    "                best_test_loss = epoch_test_loss\n",
    "                # Save the state of the best model for future reference\n",
    "                torch.save(net.state_dict(), 'backup_best_model.pth')\n",
    "                mlflow.log_metric(\"best_test_loss_so_far\", best_test_loss, step=epoch)\n",
    "                best_model_epoch = epoch\n",
    "            \n",
    "            # Implements early stopping based on a predefined tolerance and patience\n",
    "            if dif > args['tolerance']:\n",
    "                no_improvement_epochs = 0\n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= args['patience']:\n",
    "                print(\"Early stopping at epoch: \", epoch)\n",
    "                break\n",
    "\n",
    "        # Records the time at which training ends and calculates the duration\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # Clears the output and prints the final training status\n",
    "        clear_output(wait=True)\n",
    "        print(\"Neural network training has finished. Elapsed Time: {:.2f} seconds.\".format(duration))\n",
    "        print(\"Best model saved.\")\n",
    "        print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "\n",
    "        # Loads the best model\n",
    "        true_test_labels, predicted_test_labels = [], []\n",
    "        net.load_state_dict(torch.load(\"backup_best_model.pth\"))\n",
    "\n",
    "        # Uses the best model to make predictions on the test data\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                dado, rotulo = batch\n",
    "                dado, rotulo = dado.to(device), rotulo.to(device)\n",
    "                pred = net(dado)\n",
    "                true_test_labels.extend(rotulo.cpu().numpy())\n",
    "                predicted_test_labels.extend((torch.sigmoid(pred).squeeze() > 0.5).cpu().numpy()) \n",
    "                \n",
    "        # Calculates performance metrics and logs them using MLflow\n",
    "        test_accuracy = accuracy_score(true_test_labels, predicted_test_labels)\n",
    "        test_precision = precision_score(true_test_labels, predicted_test_labels)\n",
    "        test_recall = recall_score(true_test_labels, predicted_test_labels)\n",
    "        test_f1 = f1_score(true_test_labels, predicted_test_labels)\n",
    "        mlflow.pytorch.log_model(net, \"models\")\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "        mlflow.log_metric(\"duration\", duration)\n",
    "\n",
    "        # Prints the performance metrics of the best model\n",
    "        print(\"Best model metrics:\")\n",
    "        print('Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}'.format(\n",
    "            test_accuracy, test_precision, test_recall, test_f1))\n",
    "    \n",
    "    return(train_losses, test_losses, best_model_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27963c84-ead1-48fe-aae7-dbef434a40b1",
   "metadata": {},
   "source": [
    "### Setting Up MLflow Experiment Tracking\n",
    "This section initializes the experiment tracking using MLflow. The code ensures that an experiment with the name `BASIC_NN` is set up. If it doesn't already exist, a new experiment is created; otherwise, the existing experiment's ID is retrieved. This setup allows for organized tracking of different model runs and their respective metrics, parameters, and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef31940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name for MLflow tracking\n",
    "experiment_name = \"BASIC_NN\"\n",
    "\n",
    "# Check if experiment exists; create it if not\n",
    "if mlflow.get_experiment_by_name(experiment_name) is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    # Get existing experiment ID\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45e379-9d1a-40ff-a19e-3db7d502604f",
   "metadata": {},
   "source": [
    "### Parameter Grid Definition and Model Training\n",
    "\n",
    "In this section, a `grid search` approach is implemented to explore the optimal combination of hyperparameters for the neural network. Various configurations are tested through a systematic and exhaustive search over the specified hyperparameter grid, which includes parameters such as `learning rate`, `weight decay`, and `vectorization` method. The performance of each configuration is logged to identify the most promising setup for achieving superior model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09011009-00b9-4c2e-90c5-3c9594f919f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'vectorization': ['cbow', 'sg'], \n",
    "    'criterion': ['BCEWithLogitsLoss'], \n",
    "    'optimizer': ['Adam'], \n",
    "    'batch_size': [128],\n",
    "    'num_workers': [0],\n",
    "    'lr': [0.0001, 0.0005, 0.001], \n",
    "    'weight_decay': [0.0005, 0.0001, 0.001], \n",
    "    'num_epochs': [500],\n",
    "    'tolerance': [0.0003],\n",
    "    'patience': [25]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters for grid search\n",
    "all_combinations = [dict(zip(param_grid, v)) for v in product(*param_grid.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9de24df-c06f-4798-9f2e-68051a930f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models have already been trained.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove already trained combinations from the list\n",
    "def remove_trained(combinations, experiment_id):\n",
    "    # Fetch all runs from MLflow for the given experiment ID\n",
    "    runs = pd.DataFrame(mlflow.search_runs(experiment_id))\n",
    "    \n",
    "    trained_lst = []\n",
    "    not_trained_lst = []\n",
    "\n",
    "    # Loop through all runs to gather parameters of trained models\n",
    "    for i in range(runs.shape[0]):\n",
    "        run = runs.iloc[i]\n",
    "        params = {\n",
    "            'vectorization': run['params.vectorization'],\n",
    "            'criterion': run['params.criterion'],\n",
    "            'optimizer': run['params.optimizer'],\n",
    "            'batch_size': int(run['params.batch_size']),\n",
    "            'num_workers': int(run['params.num_workers']),\n",
    "            'lr': float(run['params.lr']),\n",
    "            'weight_decay': float(run['params.weight_decay']),\n",
    "            'num_epochs': int(run['params.num_epochs']),\n",
    "            'tolerance': float(run['params.tolerance']),\n",
    "            'patience': int(run['params.patience'])\n",
    "        }\n",
    "\n",
    "        trained_lst.append(params)\n",
    "\n",
    "    # Check which combinations are not yet trained\n",
    "    for i in range(len(combinations)):\n",
    "        already_trained = False\n",
    "        \n",
    "        for j in range(len(trained_lst)):\n",
    "            if combinations[i] == trained_lst[j]:\n",
    "                already_trained = True\n",
    "\n",
    "        if not already_trained:\n",
    "            not_trained_lst.append(combinations[i])\n",
    "                \n",
    "    return(not_trained_lst)\n",
    "\n",
    "# Remove already trained combinations from the list\n",
    "not_trained_lst = remove_trained(all_combinations,experiment_id)\n",
    "\n",
    "# Check if any combinations are left to train\n",
    "if len(not_trained_lst)>0:\n",
    "    print('There are {} combinations left to train.'.format(len(not_trained_lst)))\n",
    "else:\n",
    "    print('All models have already been trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "014d94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is already complete for all models.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Record the start time for the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each combination of hyperparameters that have not yet been trained\n",
    "for params in not_trained_lst:\n",
    "\n",
    "    # Select the appropriate word vector based on the 'vectorization' parameter\n",
    "    if params.get('vectorization') == 'cbow':\n",
    "        matrix_train = matrix_train_cbow.copy()\n",
    "        matrix_test = matrix_test_cbow.copy()\n",
    "\n",
    "    if params.get('vectorization') == 'sg':\n",
    "        matrix_train = matrix_train_sg.copy()\n",
    "        matrix_test = matrix_test_sg.copy()\n",
    "\n",
    "    # Train the neural network with the given parameters\n",
    "    train_losses, test_losses, best_epoch = train_nn(matrix_train, matrix_test, params,\n",
    "                                                     hidden_size=params.get('hidden_size', 128),\n",
    "                                                     input_size=params.get('input_size', 300),\n",
    "                                                     output_size=params.get('output_size', 1),)\n",
    "\n",
    "    # Append the training results to the results list\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'best_epoch': best_epoch\n",
    "    })\n",
    "\n",
    "# Record the end time and calculate the total duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "clear_output()\n",
    "\n",
    "# Display the status of the training process\n",
    "if len(not_trained_lst)>0:\n",
    "    print('Training Process Finished. Elapsed Time: {:.2f} seconds.'.format(duration))\n",
    "else:\n",
    "    print('Training is already complete for all models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b33831-dc76-4149-8839-38e7954cb7e0",
   "metadata": {},
   "source": [
    "### MLflow Experiment Retrieval and Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e8b437f-01c2-4675-ad53-e6a5be914acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Best Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>LR</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7ebf964e62b340b39745373537254f72</td>\n",
       "      <td>0.466808</td>\n",
       "      <td>0.776845</td>\n",
       "      <td>0.778136</td>\n",
       "      <td>0.782223</td>\n",
       "      <td>0.774092</td>\n",
       "      <td>1143.477632</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fab954073b3d4cf380a46cf9d9ca1fd1</td>\n",
       "      <td>0.467152</td>\n",
       "      <td>0.777014</td>\n",
       "      <td>0.777675</td>\n",
       "      <td>0.779545</td>\n",
       "      <td>0.775813</td>\n",
       "      <td>1019.219989</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>582897703bdf45ae8ed43db3aa957148</td>\n",
       "      <td>0.467782</td>\n",
       "      <td>0.775884</td>\n",
       "      <td>0.777572</td>\n",
       "      <td>0.783030</td>\n",
       "      <td>0.772190</td>\n",
       "      <td>992.770742</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0fb26383ae63431d9a1dbbfcf1f27a4b</td>\n",
       "      <td>0.468203</td>\n",
       "      <td>0.774823</td>\n",
       "      <td>0.777607</td>\n",
       "      <td>0.786896</td>\n",
       "      <td>0.768535</td>\n",
       "      <td>1683.228476</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0b674bc786554fc7bb9a1b9959f49c3d</td>\n",
       "      <td>0.468940</td>\n",
       "      <td>0.775164</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.790694</td>\n",
       "      <td>0.767065</td>\n",
       "      <td>1000.063587</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d7885b73364e4a5599797a5698ac9033</td>\n",
       "      <td>0.469571</td>\n",
       "      <td>0.774203</td>\n",
       "      <td>0.776910</td>\n",
       "      <td>0.785889</td>\n",
       "      <td>0.768133</td>\n",
       "      <td>1061.028413</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1147a8628a12428ba8644303425a2720</td>\n",
       "      <td>0.470253</td>\n",
       "      <td>0.773236</td>\n",
       "      <td>0.778999</td>\n",
       "      <td>0.798858</td>\n",
       "      <td>0.760103</td>\n",
       "      <td>990.023893</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cc04134b7488421c9aae0112c975d069</td>\n",
       "      <td>0.471287</td>\n",
       "      <td>0.772929</td>\n",
       "      <td>0.774963</td>\n",
       "      <td>0.781522</td>\n",
       "      <td>0.768512</td>\n",
       "      <td>1779.376123</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29a023c28d5d47729359aee5e58fe4d5</td>\n",
       "      <td>0.472016</td>\n",
       "      <td>0.772253</td>\n",
       "      <td>0.777199</td>\n",
       "      <td>0.794003</td>\n",
       "      <td>0.761092</td>\n",
       "      <td>2021.665417</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>85bf2cb02ee84bd5ac82fc22019287ab</td>\n",
       "      <td>0.472912</td>\n",
       "      <td>0.773011</td>\n",
       "      <td>0.773800</td>\n",
       "      <td>0.776061</td>\n",
       "      <td>0.771553</td>\n",
       "      <td>2037.909769</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6a0b9574a4b64c958a07f37e3f880ffa</td>\n",
       "      <td>0.473474</td>\n",
       "      <td>0.772676</td>\n",
       "      <td>0.775821</td>\n",
       "      <td>0.786258</td>\n",
       "      <td>0.765657</td>\n",
       "      <td>1697.457184</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09085567ff3546748861c0eaa1f76636</td>\n",
       "      <td>0.474940</td>\n",
       "      <td>0.770535</td>\n",
       "      <td>0.772082</td>\n",
       "      <td>0.776880</td>\n",
       "      <td>0.767342</td>\n",
       "      <td>1092.224862</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40e4f0c339a04e9392e7d8db4ab669e7</td>\n",
       "      <td>0.477766</td>\n",
       "      <td>0.769843</td>\n",
       "      <td>0.771869</td>\n",
       "      <td>0.778282</td>\n",
       "      <td>0.765561</td>\n",
       "      <td>907.690800</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d5f119e80e5847bdbe20bf40b18058c3</td>\n",
       "      <td>0.479062</td>\n",
       "      <td>0.768326</td>\n",
       "      <td>0.770708</td>\n",
       "      <td>0.778275</td>\n",
       "      <td>0.763286</td>\n",
       "      <td>1425.030287</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5bcf883a26e44effb26407203682efae</td>\n",
       "      <td>0.480375</td>\n",
       "      <td>0.767753</td>\n",
       "      <td>0.770547</td>\n",
       "      <td>0.779489</td>\n",
       "      <td>0.761808</td>\n",
       "      <td>1112.923453</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>03982970f045493d9254d830844f1ded</td>\n",
       "      <td>0.481349</td>\n",
       "      <td>0.767715</td>\n",
       "      <td>0.772795</td>\n",
       "      <td>0.789624</td>\n",
       "      <td>0.756668</td>\n",
       "      <td>905.402970</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6ac4daec1ca649f7acbe285b74dcb70d</td>\n",
       "      <td>0.484396</td>\n",
       "      <td>0.765114</td>\n",
       "      <td>0.765069</td>\n",
       "      <td>0.764487</td>\n",
       "      <td>0.765651</td>\n",
       "      <td>1204.827570</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>183cc7733d3e4012b169489643327f62</td>\n",
       "      <td>0.484852</td>\n",
       "      <td>0.765981</td>\n",
       "      <td>0.765648</td>\n",
       "      <td>0.764124</td>\n",
       "      <td>0.767177</td>\n",
       "      <td>1239.404788</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>25</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Run ID  Best Loss  Accuracy        F1    Recall  \\\n",
       "17  7ebf964e62b340b39745373537254f72   0.466808  0.776845  0.778136  0.782223   \n",
       "15  fab954073b3d4cf380a46cf9d9ca1fd1   0.467152  0.777014  0.777675  0.779545   \n",
       "16  582897703bdf45ae8ed43db3aa957148   0.467782  0.775884  0.777572  0.783030   \n",
       "8   0fb26383ae63431d9a1dbbfcf1f27a4b   0.468203  0.774823  0.777607  0.786896   \n",
       "13  0b674bc786554fc7bb9a1b9959f49c3d   0.468940  0.775164  0.778700  0.790694   \n",
       "7   d7885b73364e4a5599797a5698ac9033   0.469571  0.774203  0.776910  0.785889   \n",
       "4   1147a8628a12428ba8644303425a2720   0.470253  0.773236  0.778999  0.798858   \n",
       "6   cc04134b7488421c9aae0112c975d069   0.471287  0.772929  0.774963  0.781522   \n",
       "1   29a023c28d5d47729359aee5e58fe4d5   0.472016  0.772253  0.777199  0.794003   \n",
       "10  85bf2cb02ee84bd5ac82fc22019287ab   0.472912  0.773011  0.773800  0.776061   \n",
       "14  6a0b9574a4b64c958a07f37e3f880ffa   0.473474  0.772676  0.775821  0.786258   \n",
       "5   09085567ff3546748861c0eaa1f76636   0.474940  0.770535  0.772082  0.776880   \n",
       "12  40e4f0c339a04e9392e7d8db4ab669e7   0.477766  0.769843  0.771869  0.778282   \n",
       "3   d5f119e80e5847bdbe20bf40b18058c3   0.479062  0.768326  0.770708  0.778275   \n",
       "2   5bcf883a26e44effb26407203682efae   0.480375  0.767753  0.770547  0.779489   \n",
       "11  03982970f045493d9254d830844f1ded   0.481349  0.767715  0.772795  0.789624   \n",
       "0   6ac4daec1ca649f7acbe285b74dcb70d   0.484396  0.765114  0.765069  0.764487   \n",
       "9   183cc7733d3e4012b169489643327f62   0.484852  0.765981  0.765648  0.764124   \n",
       "\n",
       "    Precision     Duration Vectorization      LR Tolerance Patience  \\\n",
       "17   0.774092  1143.477632          cbow  0.0001    0.0003       25   \n",
       "15   0.775813  1019.219989          cbow  0.0001    0.0003       25   \n",
       "16   0.772190   992.770742          cbow  0.0001    0.0003       25   \n",
       "8    0.768535  1683.228476            sg  0.0001    0.0003       25   \n",
       "13   0.767065  1000.063587          cbow  0.0005    0.0003       25   \n",
       "7    0.768133  1061.028413            sg  0.0001    0.0003       25   \n",
       "4    0.760103   990.023893            sg  0.0005    0.0003       25   \n",
       "6    0.768512  1779.376123            sg  0.0001    0.0003       25   \n",
       "1    0.761092  2021.665417            sg   0.001    0.0003       25   \n",
       "10   0.771553  2037.909769          cbow   0.001    0.0003       25   \n",
       "14   0.765657  1697.457184          cbow  0.0005    0.0003       25   \n",
       "5    0.767342  1092.224862            sg  0.0005    0.0003       25   \n",
       "12   0.765561   907.690800          cbow  0.0005    0.0003       25   \n",
       "3    0.763286  1425.030287            sg  0.0005    0.0003       25   \n",
       "2    0.761808  1112.923453            sg   0.001    0.0003       25   \n",
       "11   0.756668   905.402970          cbow   0.001    0.0003       25   \n",
       "0    0.765651  1204.827570            sg   0.001    0.0003       25   \n",
       "9    0.767177  1239.404788          cbow   0.001    0.0003       25   \n",
       "\n",
       "   Weight Decay  \n",
       "17       0.0005  \n",
       "15        0.001  \n",
       "16       0.0001  \n",
       "8        0.0005  \n",
       "13       0.0001  \n",
       "7        0.0001  \n",
       "4        0.0001  \n",
       "6         0.001  \n",
       "1        0.0001  \n",
       "10       0.0001  \n",
       "14       0.0005  \n",
       "5        0.0005  \n",
       "12        0.001  \n",
       "3         0.001  \n",
       "2        0.0005  \n",
       "11       0.0005  \n",
       "0         0.001  \n",
       "9         0.001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to retrieve experiment data from MLflow and format it into a DataFrame\n",
    "def retrieve_experiments(id):\n",
    "    # Fetch all runs for the given experiment ID and convert it to a DataFrame\n",
    "    df = pd.DataFrame(mlflow.search_runs(experiment_ids=[id]))\n",
    "\n",
    "    # Select and rename the columns of interest\n",
    "    df = df[['run_id', 'metrics.best_test_loss_so_far', 'metrics.test_accuracy','metrics.test_f1','metrics.test_recall', 'metrics.test_precision',\n",
    "          'metrics.duration', 'params.vectorization', 'params.lr', 'params.tolerance', 'params.patience','params.weight_decay']]\n",
    "    df = df.rename(columns={\n",
    "        'run_id': 'Run ID', 'metrics.best_test_loss_so_far': 'Best Loss', 'metrics.test_accuracy': 'Accuracy', 'metrics.test_f1': 'F1',\n",
    "        'metrics.test_recall': 'Recall', 'metrics.test_precision': 'Precision', 'metrics.duration': 'Duration', 'params.vectorization': 'Vectorization',\n",
    "        'params.lr': 'LR', 'params.tolerance': 'Tolerance', 'params.patience': 'Patience', 'params.weight_decay': 'Weight Decay'})\n",
    "    return df\n",
    "\n",
    "# Retrieve and display the experiment data, sorted by 'Best Loss'\n",
    "df_basic = retrieve_experiments(experiment_id)\n",
    "display(df_basic.sort_values(by='Best Loss', ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f99b2-d819-41ee-8663-5a3d2454d0a2",
   "metadata": {},
   "source": [
    "## Second Experiment: LSTM Model\n",
    "\n",
    "In the second experiment, the model architecture advances to Long Short-Term Memory (LSTM) networks. These are specialized to remember information for extended periods and are particularly well-suited for sequence data like text. The experiment aims to explore whether the added complexity of LSTM layers improves the model's performance in sentiment analysis compared to the basic neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8831d2f-a281-4825-b5c6-06801539684d",
   "metadata": {},
   "source": [
    "### Word Count Analysis in Tweets\n",
    "\n",
    "In this section, the distribution of word counts in tweets is analyzed to determine an optimal value for `max_seq_length`, balancing the retention of relevant information with memory efficiency. Descriptive statistics, including the 90th and 95th percentiles, are calculated and visualized through a histogram and a boxplot, facilitating the identification of a threshold that minimizes memory wastage without losing crucial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e71fd760-3c43-49fc-b055-a0a7347148e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_words</th>\n",
       "      <td>1597564.0</td>\n",
       "      <td>7.259364</td>\n",
       "      <td>3.830664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count      mean       std  min  25%  50%   75%   90%   95%  \\\n",
       "num_words  1597564.0  7.259364  3.830664  1.0  4.0  7.0  10.0  13.0  14.0   \n",
       "\n",
       "            max  \n",
       "num_words  94.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of words in each tweet and store it in a new column 'num_words'\n",
    "tweets['num_words'] = tweets['text6'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# Generate descriptive statistics and add 90th and 95th percentiles\n",
    "describe = pd.DataFrame(tweets['num_words'].describe()).T\n",
    "percentile_90 = tweets['num_words'].quantile(0.90)\n",
    "describe['90%'] = percentile_90\n",
    "percentile_95 = tweets['num_words'].quantile(0.95)\n",
    "describe['95%'] = percentile_95\n",
    "describe = describe[[col for col in describe if col != \"max\"] + [\"max\"]]\n",
    "display(describe)\n",
    "\n",
    "# Create subplots for histogram and boxplot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram of word count per tweet on the first axis (ax1)\n",
    "ax1.hist(tweets['num_words'], bins=90)\n",
    "ax1.set_title('Histogram of Word Count per Tweet')\n",
    "ax1.set_xlabel('Word Count')\n",
    "ax1.set_ylabel('Number of Tweets')\n",
    "ax1.grid(axis='y')\n",
    "\n",
    "# Boxplot of word count per tweet on the second axis (ax2)\n",
    "ax2.boxplot(tweets['num_words'], vert=False)\n",
    "ax2.set_title('Boxplot of Word Count per Tweet')\n",
    "ax2.set_ylabel('Word Count')\n",
    "ax2.set_yticks([])\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e597f7d-d81b-4b1d-a943-8b233d0ce5ed",
   "metadata": {},
   "source": [
    "### Vectorization Upgrade: Capturing Linear Textual Nuances\n",
    "At this stage of the project, the vectorization strategy was adjusted to preserve the order of words in sentences, a vital component for sentiment analysis. Instead of simply adding up the vectors of the words to create a single vector for each sentence (as initially done), now each word is represented individually up to a defined sequence limit (max_seq_length), forming a matrix of vectors for each sentence. This method, implemented by the vectorize_sequences function, aims to capture more complex semantic nuances present in the linear structure of the text.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be6ab719-8738-4f11-8b98-433ae610c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to vectorize sequences in batches and save them to disk\n",
    "def vectorize_sequences(text, labels, modelo, batch_size=2560, max_seq_length=20, save_path=\"./\"):\n",
    "    # Initialize a WhitespaceTokenizer\n",
    "    ws_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = len(text) // batch_size + 1\n",
    "    \n",
    "    # Check if the existing number of batch files matches the number of new batches\n",
    "    existing_files = len(os.listdir(save_path))\n",
    "    if existing_files == num_batches:\n",
    "        return False # No need to create new batches\n",
    "\n",
    "    # Loop through each batch\n",
    "    for batch_num in range(num_batches):\n",
    "        # Define the start and end index for each batch\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(text))\n",
    "\n",
    "        # Extract the texts and labels for the current batch\n",
    "        batch_texts = text[start_idx:end_idx]\n",
    "        batch_labels = labels[start_idx:end_idx].values  # Convert to numpy array\n",
    "\n",
    "        # Initialize a list to store vectors for all texts in the batch\n",
    "        all_vectors = []\n",
    "\n",
    "        # Convert each text in the batch to vectors\n",
    "        for txt in batch_texts:\n",
    "            words = ws_tokenizer.tokenize(txt)\n",
    "            vectors = [modelo.wv[word] for word in words if word in modelo.wv][:max_seq_length]\n",
    "            all_vectors.append(vectors)\n",
    "\n",
    "        # Initialize a zero-matrix to store the vectors\n",
    "        matrix = np.zeros((len(all_vectors), max_seq_length, 300))\n",
    "        \n",
    "        # Fill the zero-matrix with the actual vectors\n",
    "        for i, vectors in enumerate(all_vectors):\n",
    "            for j, vector in enumerate(vectors):\n",
    "                matrix[i][j] = vector\n",
    "\n",
    "        # Combine the vectors and labels into a single dictionary\n",
    "        batch_data = {'data': matrix, 'labels': batch_labels}\n",
    "        \n",
    "        # Save the batch to disk as a .npy file\n",
    "        np.save(f\"{save_path}batch_{batch_num}.npy\", batch_data)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca2781bd-1996-4669-8031-9beb65be2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths where the training and testing batches for CBOW and Skip-gram are stored\n",
    "matrix_train_cbow = 'D:\\\\TSA_Batches\\\\cbow_train\\\\'\n",
    "matrix_test_cbow = 'D:\\\\TSA_Batches\\\\cbow_test\\\\'\n",
    "matrix_train_sg = 'D:\\\\TSA_Batches\\\\sg_train\\\\'\n",
    "matrix_test_sg = 'D:\\\\TSA_Batches\\\\sg_test\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7da9c9cc-2ea2-4968-b41a-97041b01b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBow Training Data already vectorized. Skipping vectorization.\n",
      "CBow Test Data already vectorized. Skipping vectorization.\n",
      "SG Training Data already vectorized. Skipping vectorization.\n",
      "SG Test Data already vectorized. Skipping vectorization.\n"
     ]
    }
   ],
   "source": [
    "# List of directory paths where the vectorized data will be saved\n",
    "path_lst = [matrix_train_cbow, matrix_test_cbow, matrix_train_sg, matrix_test_sg]\n",
    "\n",
    "# Descriptive names for each folder in the path list\n",
    "folder = ['CBow Training Data', 'CBow Test Data', 'SG Training Data', 'SG Test Data']\n",
    "\n",
    "# Corresponding Word2Vec models for each type of data\n",
    "model = [cbow,cbow,sg,sg]\n",
    "\n",
    "# Loop through each directory path to vectorize and save the sequences\n",
    "for i, path in enumerate(path_lst):\n",
    "    # Record the time at the start of the vectorization process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Vectorize and save training data for CBOW and Skip-gram models\n",
    "    if (folder[i]=='CBow Training Data' or folder[i]=='SG Training Data'):\n",
    "        vectorized = vectorize_sequences(X_train, y_train, model[i], save_path=path_lst[i])\n",
    "\n",
    "    # Vectorize and save test data for CBOW and Skip-gram models\n",
    "    if (folder[i]=='CBow Test Data' or folder[i]=='SG Test Data'):\n",
    "        vectorized = vectorize_sequences(X_test, y_test, model[i], save_path=path_lst[i])\n",
    "\n",
    "     # Record the time at the end of the vectorization process\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print the time elapsed during the vectorization process\n",
    "    if vectorized:\n",
    "        print('{} Vectorized. Elapsed Time: {:.2f} seconds.'.format(folder[i], (end_time - start_time)))\n",
    "    else:\n",
    "        print(\"{} already vectorized. Skipping vectorization.\".format(folder[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c83c3ef-32ae-4361-a141-507c6498b6a0",
   "metadata": {},
   "source": [
    "### Adjustments to the LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9084a102-6080-4769-bd4d-77dfc295abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Dataset class for handling batches of tweets\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        # Initialize the list of files to be loaded\n",
    "        self.files = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of files\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the data and labels from the .npy file at the given index\n",
    "        data = np.load(self.files[idx], allow_pickle=True)\n",
    "        \n",
    "        # Extract 'data' and 'labels' from the loaded file\n",
    "        # If 'data' or 'labels' is not found, default to zeros\n",
    "        vectors = torch.tensor(data.item().get('data', np.zeros((1280, 20, 300))), dtype=torch.float32)\n",
    "        labels = torch.tensor(data.item().get('labels', np.zeros(1280)), dtype=torch.float32)\n",
    "        \n",
    "        # Check that both vectors and labels are tensors\n",
    "        assert isinstance(vectors, torch.Tensor), f\"Data in {self.files[idx]} is not a tensor!\"\n",
    "        assert isinstance(labels, torch.Tensor), f\"Labels in {self.files[idx]} is not a tensor!\"\n",
    "        \n",
    "        return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f2acaf8-2150-434d-ae4e-72865dc75ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers, dropout_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, \n",
    "                            hidden_size, \n",
    "                            num_layers=n_layers, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout_prob if n_layers > 1 else 0)\n",
    "\n",
    "        # Fully connected layer for binary classification\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    # Define forward pass\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Extract the output of the last timestep for each sequence\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Forward pass through the fully connected layer\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2018f9e2-4c37-457c-a60d-003518d45eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for a single epoch\n",
    "def train(train_loader, net, epoch, criterion, optimizer):\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # Initialize list to store batch losses\n",
    "    epoch_loss = []\n",
    "\n",
    "    # Iterate through each batch in the training data loader\n",
    "    for batch in train_loader:\n",
    "        dado, rotulo = batch\n",
    "        \n",
    "        # Remove the extra dimension and move tensors to the selected device (GPU/CPU)\n",
    "        dado = dado.squeeze(0).to(device)\n",
    "        rotulo = rotulo.squeeze(0).float().to(device)  # BCELoss expects float labels\n",
    "\n",
    "        # Forward pass: compute predicted output and loss\n",
    "        pred = net(dado)\n",
    "        loss = criterion(pred, rotulo.unsqueeze(1))\n",
    "        \n",
    "        # Append the batch loss to the list of epoch losses\n",
    "        epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "        # Backward pass: compute gradient and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert epoch loss to NumPy array and compute mean\n",
    "    epoch_loss = np.asarray(epoch_loss)\n",
    "    return epoch_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41246ff8-670f-4f2a-9f3f-a368b726cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the network\n",
    "def test(test_loader, net, epoch, criterion):\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Initialize list to store batch losses\n",
    "    epoch_loss = []\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop over each batch from the test set\n",
    "        for batch in test_loader:\n",
    "            dado, rotulo = batch\n",
    "            \n",
    "            # Remove the extra dimension and move tensors to the selected device (GPU/CPU)\n",
    "            dado = dado.squeeze(0).to(device)\n",
    "            rotulo = rotulo.squeeze(0).float().to(device)  # BCELoss expects float labels\n",
    "\n",
    "            # Forward pass: compute predicted output and loss\n",
    "            pred = net(dado)\n",
    "            loss = criterion(pred, rotulo.unsqueeze(1))\n",
    "            \n",
    "            # Append the batch loss to the list of epoch losses\n",
    "            epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "    # Convert epoch loss to NumPy array and compute mean\n",
    "    epoch_loss = np.asarray(epoch_loss)\n",
    "    return epoch_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d03cdf4c-358c-4e6b-8e2e-fb487c557417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(matrix_train_path, matrix_test_path, args, hidden_size=32, input_size=300, output_size=1):\n",
    "\n",
    "    # Start MLflow run for experiment tracking\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        # Log hyperparameters\n",
    "        for key, value in args.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Log neural network architecture details\n",
    "        mlflow.log_param('input_size', input_size)\n",
    "        mlflow.log_param('hidden_size', hidden_size)\n",
    "        mlflow.log_param('output_size', output_size)\n",
    "\n",
    "        # Instantiate the model\n",
    "        net = LSTMClassifier(input_size=input_size, hidden_size=hidden_size, output_size=output_size, n_layers=2)\n",
    "        net.to(device)\n",
    "\n",
    "        # Sets the loss function\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        # Chooses the optimizer based on user input\n",
    "        if args['optimizer'] == 'SGD':\n",
    "            optimizer = optim.SGD(net.parameters(), lr=args['lr'])\n",
    "        if args['optimizer'] == 'Adam':\n",
    "            optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "        # Initializes variables to keep track of losses and other metrics\n",
    "        train_losses, test_losses = [], []\n",
    "        best_test_loss = float('inf')\n",
    "        best_model_epoch = -1\n",
    "        no_improvement_epochs = 0\n",
    "        epoch_durations = []\n",
    "\n",
    "        # Records the time at which training starts\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(args['num_epochs']):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Update and display the training status\n",
    "            clear_output(wait=True)\n",
    "            print('Training neural network... Epoch ' + str(epoch) + '/' + str(args['num_epochs']-1))\n",
    "            if len(epoch_durations) > 0:\n",
    "                # Calculates and displays the mean epoch duration and estimated time to completion\n",
    "                mean_time = np.mean(epoch_durations)\n",
    "                estimative = ((args['num_epochs'] - epoch) * mean_time)\n",
    "                print('Mean Epoch time duration: {:.2f} seconds | Estimated time to finish training: {:.2f} seconds.'.format(mean_time, estimative))\n",
    "            \n",
    "            if best_model_epoch!=-1:\n",
    "                print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "\n",
    "            \n",
    "            # Load datasets and dataloaders\n",
    "            def sort_key(s):\n",
    "                return int(re.search(r'\\d+', s).group())\n",
    "\n",
    "            # Sorts and loads the training and test batch files\n",
    "            train_file_list = [os.path.join(matrix_train_path, f) for f in os.listdir(matrix_train_path) if f.endswith('.npy')]\n",
    "            train_file_list.sort(key=sort_key)\n",
    "            \n",
    "            test_file_list = [os.path.join(matrix_test_path, f) for f in os.listdir(matrix_test_path) if f.endswith('.npy')]\n",
    "            test_file_list.sort(key=sort_key)\n",
    "\n",
    "            # Creates DataLoader objects for training and test datasets\n",
    "            train_dataset = TweetDataset(train_file_list)\n",
    "            test_dataset = TweetDataset(test_file_list)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=args['num_workers'])\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=args['num_workers'])\n",
    "\n",
    "            # Trains the model for one epoch and logs the training loss\n",
    "            epoch_train_loss = train(train_loader, net, epoch, criterion, optimizer)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            mlflow.log_metric(\"epoch_train_loss\", epoch_train_loss, step=epoch)\n",
    "\n",
    "            # Tests the model and logs the test loss\n",
    "            epoch_test_loss = test(test_loader, net, epoch, criterion)\n",
    "            test_losses.append(epoch_test_loss)\n",
    "            mlflow.log_metric(\"epoch_test_loss\", epoch_test_loss, step=epoch)\n",
    "\n",
    "            # Checks for improvements in test loss for early stopping and model checkpointing\n",
    "            dif = best_test_loss - epoch_test_loss\n",
    "            \n",
    "            # Early stopping and model checkpoint logic\n",
    "            if epoch_test_loss < best_test_loss:\n",
    "                best_test_loss = epoch_test_loss\n",
    "                # Save the state of the best model for future reference\n",
    "                torch.save(net.state_dict(), 'backup_best_model.pth')\n",
    "                mlflow.log_metric(\"best_test_loss_so_far\", best_test_loss, step=epoch)\n",
    "                best_model_epoch = epoch\n",
    "            \n",
    "            if dif > args['tolerance']:\n",
    "                no_improvement_epochs = 0\n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "                if no_improvement_epochs >= args['patience']:\n",
    "                    print(\"Early stopping at epoch: \", epoch)\n",
    "                    break\n",
    "\n",
    "            # Records the duration of each epoch for performance analysis\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_durations.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "        # Logs the overall time taken for the training process\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # Clears the output and prints the final training status\n",
    "        clear_output(wait=True)\n",
    "        print(\"Neural network training has finished. Elapsed Time: {:.2f} seconds.\".format(duration))\n",
    "        print(\"Best model saved.\")\n",
    "        print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "        \n",
    "        # Loads the best model for final evaluation\n",
    "        net.load_state_dict(torch.load(\"backup_best_model.pth\"))\n",
    "\n",
    "        # Initialize lists for true and predicted test labels\n",
    "        true_test_labels, predicted_test_labels = [], []\n",
    "\n",
    "        # Evaluates the best model on the test set\n",
    "        with torch.no_grad():\n",
    "            for data,labels in test_loader:\n",
    "                #print(\"Labels shape:\", labels.shape)\n",
    "                inputs = data.squeeze(0).to(device)\n",
    "                outputs = net(inputs)\n",
    "                predicted_labels = (outputs.squeeze() > 0.5).long()\n",
    "                true_test_labels.extend(labels.squeeze(0).cpu().numpy())\n",
    "                predicted_test_labels.extend(predicted_labels.cpu().numpy())\n",
    "                #print(len(true_test_labels), len(predicted_test_labels))\n",
    "\n",
    "        # Calculates performance metrics for the best model\n",
    "        test_accuracy = accuracy_score(true_test_labels, predicted_test_labels)\n",
    "        test_precision = precision_score(true_test_labels, predicted_test_labels)\n",
    "        test_recall = recall_score(true_test_labels, predicted_test_labels)\n",
    "        test_f1 = f1_score(true_test_labels, predicted_test_labels)\n",
    "\n",
    "        # Logs the performance metrics using MLflow\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "        mlflow.log_metric(\"duration\", sum(epoch_durations))\n",
    "\n",
    "        # Prints the performance metrics of the best model\n",
    "        print(\"Best model metrics:\")\n",
    "        print('Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}'.format(\n",
    "            test_accuracy, test_precision, test_recall, test_f1))\n",
    "    \n",
    "    return (train_losses, test_losses, best_model_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a353ce-a9e7-4f03-bbb0-0d5c21a0e778",
   "metadata": {},
   "source": [
    "### Setting Up MLflow Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "362529bb-ca71-4e28-bdfc-611cf3afd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name for MLflow tracking\n",
    "experiment_name = \"LSTM\"\n",
    "\n",
    "# Check if experiment exists; create it if not\n",
    "if mlflow.get_experiment_by_name(experiment_name) is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    # Get existing experiment ID\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd8ca5-8461-4c0d-8e83-1f3c43efbc91",
   "metadata": {},
   "source": [
    "### Parameter Grid Definition and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d877913f-de56-4c18-b14d-5683be773fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models have already been trained.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'vectorization': ['cbow', 'sg'], \n",
    "    'criterion': ['BCEWithLogitsLoss'], \n",
    "    'optimizer': ['Adam'], \n",
    "    'batch_size': [128],\n",
    "    'num_workers': [0],\n",
    "    'lr': [0.0001, 0.0005, 0.001], \n",
    "    'weight_decay': [0.0005, 0.0001, 0.001], \n",
    "    'num_epochs': [500],\n",
    "    'tolerance': [0.0003],\n",
    "    'patience': [10]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters for grid search\n",
    "all_combinations = [dict(zip(param_grid, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Remove already trained combinations from the list\n",
    "not_trained_lst = remove_trained(all_combinations,experiment_id)\n",
    "\n",
    "# Check if any combinations are left to train\n",
    "if len(not_trained_lst)>0:\n",
    "    print('There are {} combinations left to train.'.format(len(not_trained_lst)))\n",
    "else:\n",
    "    print('All models have already been trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a10c589-d7a4-4014-a048-39391ba24914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is already complete for all models.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Record the start time for the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each combination of hyperparameters that have not yet been trained\n",
    "for params in not_trained_lst:\n",
    "\n",
    "    # Select the appropriate word vector based on the 'vectorization' parameter and train the neural network with the given parameters\n",
    "    if params.get('vectorization') == 'cbow':\n",
    "        train_losses, test_losses, best_epoch = train_LSTM(matrix_train_cbow, matrix_test_cbow, params, hidden_size=params.get('hidden_size', 256), input_size=params.get('input_size', 300), output_size=params.get('output_size', 1))\n",
    "\n",
    "    else:\n",
    "        train_losses, test_losses, best_epoch = train_LSTM(matrix_train_sg, matrix_test_sg, params, hidden_size=params.get('hidden_size', 256), input_size=params.get('input_size', 300), output_size=params.get('output_size', 1))\n",
    "\n",
    "    # Append the training results to the results list\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'best_epoch': best_epoch\n",
    "    })\n",
    "\n",
    "# Record the end time and calculate the total duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "clear_output()\n",
    "\n",
    "# Display the status of the training process\n",
    "if len(not_trained_lst)>0:\n",
    "    print('Training Process Finished. Elapsed Time: {:.2f} seconds.'.format(duration))\n",
    "else:\n",
    "    print('Training is already complete for all models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87865448-aaad-47af-9ed7-3357a2d49474",
   "metadata": {},
   "source": [
    "### MLflow Experiment Retrieval and Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79d90bf8-a8fe-4f4d-9f5a-ff28aaa6c683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Best Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>LR</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>d72f8f3fcad84255838c99a880f7e0ec</td>\n",
       "      <td>0.436744</td>\n",
       "      <td>0.783693</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.841794</td>\n",
       "      <td>23631.324723</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>148c1ba149bf4e90b7a5daae3de36ebd</td>\n",
       "      <td>0.437911</td>\n",
       "      <td>0.781558</td>\n",
       "      <td>0.760427</td>\n",
       "      <td>0.692963</td>\n",
       "      <td>0.842445</td>\n",
       "      <td>17468.263312</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3e4e75ef579848c786f8af2c19b64863</td>\n",
       "      <td>0.440402</td>\n",
       "      <td>0.782910</td>\n",
       "      <td>0.765658</td>\n",
       "      <td>0.708890</td>\n",
       "      <td>0.832310</td>\n",
       "      <td>23987.815740</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bd81602a60334fdba038b34f1ee80bfa</td>\n",
       "      <td>0.441892</td>\n",
       "      <td>0.782641</td>\n",
       "      <td>0.765063</td>\n",
       "      <td>0.707420</td>\n",
       "      <td>0.832933</td>\n",
       "      <td>33972.275484</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>99ca3c62e73e4d99905353a7cc2a96eb</td>\n",
       "      <td>0.442978</td>\n",
       "      <td>0.778820</td>\n",
       "      <td>0.757903</td>\n",
       "      <td>0.692031</td>\n",
       "      <td>0.837636</td>\n",
       "      <td>36729.734913</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9e208bed79b343779265421c45cc615b</td>\n",
       "      <td>0.443190</td>\n",
       "      <td>0.781608</td>\n",
       "      <td>0.763868</td>\n",
       "      <td>0.706081</td>\n",
       "      <td>0.831957</td>\n",
       "      <td>51131.960991</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>f82d81b9f18048b8b4de1e345d0db4b3</td>\n",
       "      <td>0.446117</td>\n",
       "      <td>0.779186</td>\n",
       "      <td>0.760530</td>\n",
       "      <td>0.700883</td>\n",
       "      <td>0.831273</td>\n",
       "      <td>42153.247715</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>74d9ff0b7cf04abcb3083f10617720f2</td>\n",
       "      <td>0.446320</td>\n",
       "      <td>0.777108</td>\n",
       "      <td>0.755791</td>\n",
       "      <td>0.689428</td>\n",
       "      <td>0.836290</td>\n",
       "      <td>24804.346572</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>294387351fac4b08b109a04c643e084d</td>\n",
       "      <td>0.693146</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9197.926338</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029d485eab38455a9881e60063d8316a</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12124.521789</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ec0dc3f199549de9c92a18f9c81dc43</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19112.672390</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004a2af31d3447259170c450a676c326</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8913.181664</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fae9b4fa947d42169fd65d24370f577c</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8520.620106</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c77a639a30c9423b96e38e3b6e727ab5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8150.455637</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4028a07adb6401eaf3d325c56b77e47</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11260.030783</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8ada4e0933d54a7dbe8e56e85c2d329c</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8554.694114</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9fae989fc60419baa05598d897beff5</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11727.595294</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0a90287e7be347ae87fe54c04def2e61</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9365.448078</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Run ID  Best Loss  Accuracy        F1    Recall  \\\n",
       "14  d72f8f3fcad84255838c99a880f7e0ec   0.436744  0.783693  0.763780  0.699000   \n",
       "13  148c1ba149bf4e90b7a5daae3de36ebd   0.437911  0.781558  0.760427  0.692963   \n",
       "16  3e4e75ef579848c786f8af2c19b64863   0.440402  0.782910  0.765658  0.708890   \n",
       "11  bd81602a60334fdba038b34f1ee80bfa   0.441892  0.782641  0.765063  0.707420   \n",
       "10  99ca3c62e73e4d99905353a7cc2a96eb   0.442978  0.778820  0.757903  0.692031   \n",
       "17  9e208bed79b343779265421c45cc615b   0.443190  0.781608  0.763868  0.706081   \n",
       "12  f82d81b9f18048b8b4de1e345d0db4b3   0.446117  0.779186  0.760530  0.700883   \n",
       "15  74d9ff0b7cf04abcb3083f10617720f2   0.446320  0.777108  0.755791  0.689428   \n",
       "9   294387351fac4b08b109a04c643e084d   0.693146  0.499717  0.000000  0.000000   \n",
       "3   029d485eab38455a9881e60063d8316a   0.693147  0.499717  0.000000  0.000000   \n",
       "4   0ec0dc3f199549de9c92a18f9c81dc43   0.693147  0.499717  0.000000  0.000000   \n",
       "5   004a2af31d3447259170c450a676c326   0.693147  0.499717  0.000000  0.000000   \n",
       "7   fae9b4fa947d42169fd65d24370f577c   0.693147  0.499717  0.000000  0.000000   \n",
       "0   c77a639a30c9423b96e38e3b6e727ab5   0.693147  0.499717  0.000000  0.000000   \n",
       "2   a4028a07adb6401eaf3d325c56b77e47   0.693147  0.499717  0.000000  0.000000   \n",
       "6   8ada4e0933d54a7dbe8e56e85c2d329c   0.693147  0.499717  0.000000  0.000000   \n",
       "1   c9fae989fc60419baa05598d897beff5   0.693147  0.499717  0.000000  0.000000   \n",
       "8   0a90287e7be347ae87fe54c04def2e61   0.693147  0.499717  0.000000  0.000000   \n",
       "\n",
       "    Precision      Duration Vectorization      LR Tolerance Patience  \\\n",
       "14   0.841794  23631.324723          cbow  0.0005    0.0003       10   \n",
       "13   0.842445  17468.263312          cbow   0.001    0.0003       10   \n",
       "16   0.832310  23987.815740          cbow  0.0001    0.0003       10   \n",
       "11   0.832933  33972.275484            sg  0.0005    0.0003       10   \n",
       "10   0.837636  36729.734913            sg   0.001    0.0003       10   \n",
       "17   0.831957  51131.960991          cbow  0.0001    0.0003       10   \n",
       "12   0.831273  42153.247715            sg  0.0001    0.0003       10   \n",
       "15   0.836290  24804.346572          cbow  0.0005    0.0003       10   \n",
       "9    0.000000   9197.926338          cbow  0.0001    0.0003       10   \n",
       "3    0.000000  12124.521789            sg  0.0005    0.0003       10   \n",
       "4    0.000000  19112.672390            sg  0.0001    0.0003       10   \n",
       "5    0.000000   8913.181664            sg  0.0001    0.0003       10   \n",
       "7    0.000000   8520.620106          cbow   0.001    0.0003       10   \n",
       "0    0.000000   8150.455637            sg   0.001    0.0003       10   \n",
       "2    0.000000  11260.030783            sg  0.0005    0.0003       10   \n",
       "6    0.000000   8554.694114          cbow   0.001    0.0003       10   \n",
       "1    0.000000  11727.595294            sg   0.001    0.0003       10   \n",
       "8    0.000000   9365.448078          cbow  0.0005    0.0003       10   \n",
       "\n",
       "   Weight Decay  \n",
       "14       0.0001  \n",
       "13       0.0001  \n",
       "16       0.0001  \n",
       "11       0.0001  \n",
       "10       0.0001  \n",
       "17       0.0005  \n",
       "12       0.0001  \n",
       "15       0.0005  \n",
       "9         0.001  \n",
       "3        0.0005  \n",
       "4         0.001  \n",
       "5        0.0005  \n",
       "7        0.0005  \n",
       "0         0.001  \n",
       "2         0.001  \n",
       "6         0.001  \n",
       "1        0.0005  \n",
       "8         0.001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve and display the experiment data, sorted by 'Best Loss'\n",
    "df_lstm = retrieve_experiments(experiment_id)\n",
    "display(df_lstm.sort_values(by='Best Loss', ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cacb01-9c9b-4e47-8eec-d7f7163fdb61",
   "metadata": {},
   "source": [
    "## Third Experiment: GRU Model\n",
    "\n",
    "In the third experiment, the architecture transitions to Gated Recurrent Units (GRU), a variant of LSTM networks. GRUs offer a simpler computational model while retaining the ability to capture long-term dependencies in sequence data. The experiment seeks to determine whether GRU-based models can offer a balance between complexity and performance in sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161113c-77be-4523-a228-ee1adfb8dc36",
   "metadata": {},
   "source": [
    "### Adjustments to the GRU Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e71a7beb-0ec9-4009-94af-d51c86ba8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU-based classifier model\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers, dropout_prob=0.5):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        \n",
    "        # Initialize the GRU layer with the given dimensions and dropout rate.\n",
    "        # If there's more than one layer, apply dropout; otherwise, don't.\n",
    "        self.gru = nn.GRU(input_size, \n",
    "                       hidden_size, \n",
    "                       num_layers=n_layers, \n",
    "                       batch_first=True, \n",
    "                       dropout=dropout_prob if n_layers > 1 else 0)\n",
    "        \n",
    "        # Initialize the fully connected (linear) layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Initialize the dropout layer with the given dropout rate\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the GRU layer\n",
    "        # h_n is the hidden state for the last time step across all layers\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        \n",
    "        # Take the GRU output from the last time step for all samples in the batch\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "\n",
    "        # Apply dropout to the GRU output to prevent overfitting\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # Forward pass through the fully connected layer to get the final output\n",
    "        output = self.fc(gru_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5604edda-be41-47ef-a97f-380aa05d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GRU(matrix_train_path, matrix_test_path, args, hidden_size=32, input_size=300, output_size=1):\n",
    "\n",
    "    # Start MLflow run for experiment tracking\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        # Log hyperparameters\n",
    "        for key, value in args.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Log neural network architecture details\n",
    "        mlflow.log_param('input_size', input_size)\n",
    "        mlflow.log_param('hidden_size', hidden_size)\n",
    "        mlflow.log_param('output_size', output_size)\n",
    "\n",
    "        # Instantiate the model\n",
    "        net = GRUClassifier(input_size=input_size, hidden_size=hidden_size, output_size=output_size, n_layers=2)\n",
    "        net.to(device)\n",
    "        \n",
    "        # Sets the loss function\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        # Chooses the optimizer based on user input\n",
    "        if args['optimizer'] == 'SGD':\n",
    "            optimizer = optim.SGD(net.parameters(), lr=args['lr'])\n",
    "        \n",
    "        if args['optimizer'] == 'Adam':\n",
    "            optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "        # Initializes variables to keep track of losses and other metrics\n",
    "        train_losses, test_losses = [], []\n",
    "        best_test_loss = float('inf')\n",
    "        best_model_epoch = -1\n",
    "        no_improvement_epochs = 0\n",
    "        epoch_durations = []\n",
    "\n",
    "        # Records the time at which training starts\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(args['num_epochs']):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Update and display the training status\n",
    "            clear_output(wait=True)\n",
    "            print('Training neural network... Epoch ' + str(epoch) + '/' + str(args['num_epochs']-1))\n",
    "\n",
    "            if len(epoch_durations) > 0:\n",
    "                # Calculates and displays the mean epoch duration and estimated time to completion\n",
    "                mean_time = np.mean(epoch_durations)\n",
    "                estimative = ((args['num_epochs'] - epoch) * mean_time)\n",
    "                print('Mean Epoch time duration: {:.2f} seconds | Estimated time to finish training: {:.2f} seconds.'.format(mean_time, estimative))\n",
    "            \n",
    "            if best_model_epoch!=-1:\n",
    "                print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "\n",
    "            # Load datasets and dataloaders\n",
    "            def sort_key(s):\n",
    "                return int(re.search(r'\\d+', s).group())\n",
    "\n",
    "            # Sorts and loads the training and test batch files\n",
    "            train_file_list = [os.path.join(matrix_train_path, f) for f in os.listdir(matrix_train_path) if f.endswith('.npy')]\n",
    "            train_file_list.sort(key=sort_key)\n",
    "            \n",
    "            test_file_list = [os.path.join(matrix_test_path, f) for f in os.listdir(matrix_test_path) if f.endswith('.npy')]\n",
    "            test_file_list.sort(key=sort_key)\n",
    "\n",
    "            # Creates DataLoader objects for training and test datasets\n",
    "            train_dataset = TweetDataset(train_file_list)\n",
    "            test_dataset = TweetDataset(test_file_list)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=args['num_workers'])\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=args['num_workers'])\n",
    "\n",
    "            # Trains the model for one epoch and logs the training loss\n",
    "            epoch_train_loss = train(train_loader, net, epoch, criterion, optimizer)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            mlflow.log_metric(\"epoch_train_loss\", epoch_train_loss, step=epoch)\n",
    "\n",
    "            # Tests the model and logs the test loss\n",
    "            epoch_test_loss = test(test_loader, net, epoch, criterion)\n",
    "            test_losses.append(epoch_test_loss)\n",
    "            mlflow.log_metric(\"epoch_test_loss\", epoch_test_loss, step=epoch)\n",
    "\n",
    "            # Checks for improvements in test loss for early stopping and model checkpointing\n",
    "            dif = best_test_loss - epoch_test_loss\n",
    "            \n",
    "            # Early stopping and model checkpoint logic\n",
    "            if epoch_test_loss < best_test_loss:\n",
    "                best_test_loss = epoch_test_loss\n",
    "                # Save the state of the best model for future reference\n",
    "                torch.save(net.state_dict(), 'backup_best_model.pth')\n",
    "                mlflow.log_metric(\"best_test_loss_so_far\", best_test_loss, step=epoch)\n",
    "                best_model_epoch = epoch\n",
    "                #no_improvement_epochs = 0\n",
    "            \n",
    "            if dif > args['tolerance']:\n",
    "                no_improvement_epochs = 0\n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "                if no_improvement_epochs >= args['patience']:\n",
    "                    print(\"Early stopping at epoch: \", epoch)\n",
    "                    break\n",
    "\n",
    "            # Records the duration of each epoch for performance analysis\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_durations.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "        # Logs the overall time taken for the training process\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # Clears the output and prints the final training status\n",
    "        clear_output(wait=True)\n",
    "        print(\"Neural network training has finished. Elapsed Time: {:.2f} seconds.\".format(duration))\n",
    "        print(\"Best model saved.\")\n",
    "        print(\"Best test loss epoch: \" + str(best_model_epoch))\n",
    "        \n",
    "        # Loads the best model for final evaluation\n",
    "        net.load_state_dict(torch.load(\"backup_best_model.pth\"))\n",
    "\n",
    "        # Initialize lists for true and predicted test labels\n",
    "        true_test_labels, predicted_test_labels = [], []\n",
    "\n",
    "        # Evaluates the best model on the test set\n",
    "        with torch.no_grad():\n",
    "            for data,labels in test_loader:\n",
    "                inputs = data.squeeze(0).to(device)\n",
    "                outputs = net(inputs)\n",
    "                predicted_labels = (outputs.squeeze() > 0.5).long()\n",
    "                true_test_labels.extend(labels.squeeze(0).cpu().numpy())\n",
    "                predicted_test_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        # Calculates performance metrics for the best model\n",
    "        test_accuracy = accuracy_score(true_test_labels, predicted_test_labels)\n",
    "        test_precision = precision_score(true_test_labels, predicted_test_labels)\n",
    "        test_recall = recall_score(true_test_labels, predicted_test_labels)\n",
    "        test_f1 = f1_score(true_test_labels, predicted_test_labels)\n",
    "\n",
    "        # Logs the performance metrics using MLflow\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "        mlflow.log_metric(\"duration\", sum(epoch_durations))\n",
    "\n",
    "        # Prints the performance metrics of the best model\n",
    "        print(\"Best model metrics:\")\n",
    "        print('Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}'.format(\n",
    "            test_accuracy, test_precision, test_recall, test_f1))\n",
    "    \n",
    "    return (train_losses, test_losses, best_model_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa4814-d466-4a4b-8a96-983448e6a1ab",
   "metadata": {},
   "source": [
    "### Parameter Grid Definition and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d517041-f1bd-4006-beb7-e05de631c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name for MLflow tracking\n",
    "experiment_name = \"GRU\"\n",
    "\n",
    "# Check if experiment exists; create it if not\n",
    "if mlflow.get_experiment_by_name(experiment_name) is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    # Get existing experiment ID\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff23083a-6181-49b5-84cd-f366e3e43f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models have already been trained.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'vectorization': ['cbow', 'sg'], \n",
    "    'criterion': ['BCEWithLogitsLoss'], \n",
    "    'optimizer': ['Adam'], \n",
    "    'batch_size': [128],\n",
    "    'num_workers': [0],\n",
    "    'lr': [0.0001, 0.0005, 0.001], \n",
    "    'weight_decay': [0.0005, 0.0001, 0.001], \n",
    "    'num_epochs': [500],\n",
    "    'tolerance': [0.0003],\n",
    "    'patience': [10]\n",
    "}\n",
    "# Generate all combinations of hyperparameters for grid search\n",
    "all_combinations = [dict(zip(param_grid, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Remove already trained combinations from the list\n",
    "not_trained_lst = remove_trained(all_combinations,experiment_id)\n",
    "\n",
    "# Check if any combinations are left to train\n",
    "if len(not_trained_lst)>0:\n",
    "    print('There are {} combinations left to train.'.format(len(not_trained_lst)))\n",
    "else:\n",
    "    print('All models have already been trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23f44a92-5242-43b8-be19-6d0a825eb677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is already complete for all models.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Record the start time for the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each combination of hyperparameters that have not yet been trained\n",
    "for params in not_trained_lst:\n",
    "\n",
    "    # Select the appropriate word vector based on the 'vectorization' parameter and train the neural network with the given parameters\n",
    "    if params.get('vectorization') == 'cbow':\n",
    "        train_losses, test_losses, best_epoch = train_GRU(matrix_train_cbow, matrix_test_cbow, params, hidden_size=params.get('hidden_size', 256), input_size=params.get('input_size', 300), output_size=params.get('output_size', 1))\n",
    "\n",
    "    else:\n",
    "        train_losses, test_losses, best_epoch = train_GRU(matrix_train_sg, matrix_test_sg, params, hidden_size=params.get('hidden_size', 256), input_size=params.get('input_size', 300), output_size=params.get('output_size', 1))\n",
    "\n",
    "    # Append the training results to the results list\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'best_epoch': best_epoch\n",
    "    })\n",
    "\n",
    "# Record the end time and calculate the total duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "clear_output()\n",
    "\n",
    "# Display the status of the training process\n",
    "if len(not_trained_lst)>0:\n",
    "    print('Training Process Finished. Elapsed Time: {:.2f} seconds.'.format(duration))\n",
    "else:\n",
    "    print('Training is already complete for all models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1cfe1-de49-498a-89d0-5409b769d80c",
   "metadata": {},
   "source": [
    "### MLflow Experiment Retrieval and Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "def3665e-a756-4aa7-9b0b-e075e62b7b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Best Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>LR</th>\n",
       "      <th>Tolerance</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fb56b00a4f0f4ae6b001d5ee3d155996</td>\n",
       "      <td>0.436536</td>\n",
       "      <td>0.783580</td>\n",
       "      <td>0.764470</td>\n",
       "      <td>0.702046</td>\n",
       "      <td>0.839078</td>\n",
       "      <td>24209.433861</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c4b9b3fe550c4915830022b0f3ab420b</td>\n",
       "      <td>0.437911</td>\n",
       "      <td>0.781830</td>\n",
       "      <td>0.761596</td>\n",
       "      <td>0.696560</td>\n",
       "      <td>0.840027</td>\n",
       "      <td>38000.263731</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>241f99a2db554eabbdd3c33113703c1d</td>\n",
       "      <td>0.440918</td>\n",
       "      <td>0.779449</td>\n",
       "      <td>0.757090</td>\n",
       "      <td>0.687013</td>\n",
       "      <td>0.843086</td>\n",
       "      <td>62607.626496</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c14b798e3bf84d52a4a0b4675262a60e</td>\n",
       "      <td>0.441350</td>\n",
       "      <td>0.779292</td>\n",
       "      <td>0.757376</td>\n",
       "      <td>0.688571</td>\n",
       "      <td>0.841457</td>\n",
       "      <td>45175.305805</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a9e10d5053534ff3a6b92d2e6687ffb2</td>\n",
       "      <td>0.441539</td>\n",
       "      <td>0.785389</td>\n",
       "      <td>0.770823</td>\n",
       "      <td>0.721421</td>\n",
       "      <td>0.827487</td>\n",
       "      <td>23293.767386</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a3708d02bf084a5bbfb86ee13276de12</td>\n",
       "      <td>0.445336</td>\n",
       "      <td>0.775380</td>\n",
       "      <td>0.751504</td>\n",
       "      <td>0.678912</td>\n",
       "      <td>0.841478</td>\n",
       "      <td>43740.106391</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29b3830f4cae4c2e9af06bac30a331d3</td>\n",
       "      <td>0.446994</td>\n",
       "      <td>0.773665</td>\n",
       "      <td>0.748640</td>\n",
       "      <td>0.673726</td>\n",
       "      <td>0.842299</td>\n",
       "      <td>29875.553173</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38535ed12a7e424bb06477a2431321ac</td>\n",
       "      <td>0.447434</td>\n",
       "      <td>0.778738</td>\n",
       "      <td>0.759951</td>\n",
       "      <td>0.700076</td>\n",
       "      <td>0.831025</td>\n",
       "      <td>19664.968760</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>786a0d143a004400a4ec54f53c1b871f</td>\n",
       "      <td>0.448812</td>\n",
       "      <td>0.779252</td>\n",
       "      <td>0.761647</td>\n",
       "      <td>0.704993</td>\n",
       "      <td>0.828202</td>\n",
       "      <td>55645.610807</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>acb2c6a733b245e2bfb85d955adf3183</td>\n",
       "      <td>0.450839</td>\n",
       "      <td>0.774410</td>\n",
       "      <td>0.752599</td>\n",
       "      <td>0.685862</td>\n",
       "      <td>0.833724</td>\n",
       "      <td>37707.681145</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b70e7722b903401a91d8da199d059a66</td>\n",
       "      <td>0.451073</td>\n",
       "      <td>0.775371</td>\n",
       "      <td>0.755231</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>35452.587542</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22f3aa320d244e6e93a156048b935caa</td>\n",
       "      <td>0.455576</td>\n",
       "      <td>0.770388</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.675771</td>\n",
       "      <td>0.833764</td>\n",
       "      <td>72345.904318</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53152013906148c69c875bd7316562ed</td>\n",
       "      <td>0.456736</td>\n",
       "      <td>0.769671</td>\n",
       "      <td>0.745869</td>\n",
       "      <td>0.675627</td>\n",
       "      <td>0.832411</td>\n",
       "      <td>121803.032488</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e68680be4f354527b1bad967997e817b</td>\n",
       "      <td>0.456811</td>\n",
       "      <td>0.771305</td>\n",
       "      <td>0.749087</td>\n",
       "      <td>0.682371</td>\n",
       "      <td>0.830263</td>\n",
       "      <td>17000.242563</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5783da039ec14d808f4b4b62db2bc2b7</td>\n",
       "      <td>0.457688</td>\n",
       "      <td>0.770185</td>\n",
       "      <td>0.747688</td>\n",
       "      <td>0.680638</td>\n",
       "      <td>0.829392</td>\n",
       "      <td>44467.771913</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>656c39f2819c4917af37eacb8475b7d1</td>\n",
       "      <td>0.693143</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9701.890003</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5292078f2684650bdc15b385bdfca34</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8553.338111</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106c02cb3aa4421d85396e004c54f0b4</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.499717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7500.003362</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Run ID  Best Loss  Accuracy        F1    Recall  \\\n",
       "13  fb56b00a4f0f4ae6b001d5ee3d155996   0.436536  0.783580  0.764470  0.702046   \n",
       "10  c4b9b3fe550c4915830022b0f3ab420b   0.437911  0.781830  0.761596  0.696560   \n",
       "5   241f99a2db554eabbdd3c33113703c1d   0.440918  0.779449  0.757090  0.687013   \n",
       "3   c14b798e3bf84d52a4a0b4675262a60e   0.441350  0.779292  0.757376  0.688571   \n",
       "16  a9e10d5053534ff3a6b92d2e6687ffb2   0.441539  0.785389  0.770823  0.721421   \n",
       "17  a3708d02bf084a5bbfb86ee13276de12   0.445336  0.775380  0.751504  0.678912   \n",
       "11  29b3830f4cae4c2e9af06bac30a331d3   0.446994  0.773665  0.748640  0.673726   \n",
       "14  38535ed12a7e424bb06477a2431321ac   0.447434  0.778738  0.759951  0.700076   \n",
       "7   786a0d143a004400a4ec54f53c1b871f   0.448812  0.779252  0.761647  0.704993   \n",
       "15  acb2c6a733b245e2bfb85d955adf3183   0.450839  0.774410  0.752599  0.685862   \n",
       "12  b70e7722b903401a91d8da199d059a66   0.451073  0.775371  0.755231  0.692700   \n",
       "8   22f3aa320d244e6e93a156048b935caa   0.455576  0.770388  0.746500  0.675771   \n",
       "4   53152013906148c69c875bd7316562ed   0.456736  0.769671  0.745869  0.675627   \n",
       "9   e68680be4f354527b1bad967997e817b   0.456811  0.771305  0.749087  0.682371   \n",
       "6   5783da039ec14d808f4b4b62db2bc2b7   0.457688  0.770185  0.747688  0.680638   \n",
       "2   656c39f2819c4917af37eacb8475b7d1   0.693143  0.499717  0.000000  0.000000   \n",
       "0   e5292078f2684650bdc15b385bdfca34   0.693147  0.499717  0.000000  0.000000   \n",
       "1   106c02cb3aa4421d85396e004c54f0b4   0.693147  0.499717  0.000000  0.000000   \n",
       "\n",
       "    Precision       Duration Vectorization      LR Tolerance Patience  \\\n",
       "13   0.839078   24209.433861          cbow  0.0005    0.0003       10   \n",
       "10   0.840027   38000.263731          cbow   0.001    0.0003       10   \n",
       "5    0.843086   62607.626496            sg  0.0005    0.0003       10   \n",
       "3    0.841457   45175.305805            sg   0.001    0.0003       10   \n",
       "16   0.827487   23293.767386          cbow  0.0001    0.0003       10   \n",
       "17   0.841478   43740.106391          cbow  0.0001    0.0003       10   \n",
       "11   0.842299   29875.553173          cbow   0.001    0.0003       10   \n",
       "14   0.831025   19664.968760          cbow  0.0005    0.0003       10   \n",
       "7    0.828202   55645.610807            sg  0.0001    0.0003       10   \n",
       "15   0.833724   37707.681145          cbow  0.0001    0.0003       10   \n",
       "12   0.830173   35452.587542          cbow  0.0005    0.0003       10   \n",
       "8    0.833764   72345.904318            sg  0.0001    0.0003       10   \n",
       "4    0.832411  121803.032488            sg   0.001    0.0003       10   \n",
       "9    0.830263   17000.242563          cbow   0.001    0.0003       10   \n",
       "6    0.829392   44467.771913            sg  0.0005    0.0003       10   \n",
       "2    0.000000    9701.890003            sg  0.0001    0.0003       10   \n",
       "0    0.000000    8553.338111            sg   0.001    0.0003       10   \n",
       "1    0.000000    7500.003362            sg  0.0005    0.0003       10   \n",
       "\n",
       "   Weight Decay  \n",
       "13       0.0001  \n",
       "10       0.0001  \n",
       "5        0.0001  \n",
       "3        0.0001  \n",
       "16       0.0001  \n",
       "17       0.0005  \n",
       "11       0.0005  \n",
       "14       0.0005  \n",
       "7        0.0001  \n",
       "15        0.001  \n",
       "12        0.001  \n",
       "8        0.0005  \n",
       "4        0.0005  \n",
       "9         0.001  \n",
       "6        0.0005  \n",
       "2         0.001  \n",
       "0         0.001  \n",
       "1         0.001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve and display the experiment data, sorted by 'Best Loss'\n",
    "df_gru = retrieve_experiments(experiment_id)\n",
    "display(df_gru.sort_values(by='Best Loss', ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e387e4a-acd1-4eb5-94bd-cd88e35f87eb",
   "metadata": {},
   "source": [
    "## Statistical Analysis and Model Selection Insights\n",
    "\n",
    "This section provides a rigorous statistical analysis aimed at validating the observed performance differences among the Basic, GRU, and LSTM model architectures. Various tests were conducted to assess not only the 'Best Loss' performance metrics but also the computational efficiency in terms of training time. The Shapiro-Wilk test confirmed that the 'Best Loss' data for each architecture approximately follow a normal distribution, thereby justifying the use of parametric tests for those metrics. However, due to the non-normal distribution of training times, non-parametric tests were employed to evaluate those differences.\r\n",
    "\r\n",
    "Statistical tests like ANOVA and the t-test revealed significant differences in 'Best Loss' between the architectures, while the Kruskal-Wallis and Mann-Whitney tests were used to understand the differences in training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae69a6-df42-4449-a253-f18e2ddde4dc",
   "metadata": {},
   "source": [
    "### Distribution Analysis of Model Performances by Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bca77d7-a86b-441d-a5ee-29fadb19aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an 'Architecture' column to each DataFrame to categorize the type of neural network used\n",
    "df_basic['Architecture'] = 'Basic'\n",
    "df_lstm['Architecture'] = 'LSTM'\n",
    "df_gru['Architecture'] = 'GRU'\n",
    "\n",
    "# Sort the DataFrames based on the 'Best Loss' column in ascending order\n",
    "# This helps to identify the best models in each category\n",
    "basic_models = df_basic.sort_values(by='Best Loss', ascending=True)\n",
    "lstm_models = df_lstm.sort_values(by='Best Loss', ascending=True)\n",
    "gru_models = df_gru.sort_values(by='Best Loss', ascending=True)\n",
    "\n",
    "# Concatenate the sorted DataFrames to create a comprehensive DataFrame\n",
    "# This DataFrame will contain models from all architectures, sorted by their performance\n",
    "final_results = pd.concat([basic_models, lstm_models, gru_models], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e73c25c6-b829-4090-8db4-f0027dce3d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.473952</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.466808</td>\n",
       "      <td>0.469098</td>\n",
       "      <td>0.472464</td>\n",
       "      <td>0.478738</td>\n",
       "      <td>0.484852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.488611</td>\n",
       "      <td>0.094341</td>\n",
       "      <td>0.436536</td>\n",
       "      <td>0.442488</td>\n",
       "      <td>0.449825</td>\n",
       "      <td>0.456792</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.581501</td>\n",
       "      <td>0.128462</td>\n",
       "      <td>0.436744</td>\n",
       "      <td>0.443031</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std       min       25%       50%  \\\n",
       "Architecture                                                            \n",
       "Basic          18.0  0.473952  0.005961  0.466808  0.469098  0.472464   \n",
       "GRU            18.0  0.488611  0.094341  0.436536  0.442488  0.449825   \n",
       "LSTM           18.0  0.581501  0.128462  0.436744  0.443031  0.693147   \n",
       "\n",
       "                   75%       max  \n",
       "Architecture                      \n",
       "Basic         0.478738  0.484852  \n",
       "GRU           0.456792  0.693147  \n",
       "LSTM          0.693147  0.693147  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displays descriptive statistics of the 'Best Loss' column, grouped by 'Architecture'.\n",
    "# This provides measures like mean, standard deviation, minimum, 25th percentile, etc., for each 'Architecture' group.\n",
    "display(final_results.groupby('Architecture').describe()['Best Loss'])\n",
    "\n",
    "# Generates a scatter plot that visualizes the 'Best Loss' for different neural network architectures.\n",
    "# The x-axis represents the 'Best Loss' values, while the y-axis represents the different architectures.\n",
    "# Each point on the plot is colored based on its architecture, and the size of each point is set to 100.\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.scatterplot(x='Best Loss', y='Architecture', hue='Architecture', data=final_results, palette='Set1', s=100, legend=False)\n",
    "plt.title('Scatterplot of Best Loss by Architecture')\n",
    "plt.xlabel('Best Loss')\n",
    "plt.ylabel('Architecture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad568a60-09e9-46b6-8cbe-7811f758754b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Failed Models</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architecture</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Number of Failed Models\n",
       "Architecture                         \n",
       "LSTM                               10\n",
       "GRU                                 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filters the models with a 'Best Loss' greater than 0.5 and stores them in 'failed_models'.\n",
    "# Then, it counts the number of failed models for each architecture and displays this information.\n",
    "failed_models = final_results[final_results['Best Loss']>0.5]\n",
    "failed_df = pd.DataFrame(failed_models['Architecture'].value_counts())\n",
    "failed_df.columns = ['Number of Failed Models']\n",
    "display(failed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042786d5-927f-43a3-ae9f-b62e50884198",
   "metadata": {},
   "source": [
    "The scatterplot above reveals a notable trend: several models from the LSTM and GRU architectures did not converge. This lack of convergence can be attributed to the complexity inherent to LSTM and GRU networks, making them more challenging to optimize. In contrast, the Basic architecture successfully converged in all instances.\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb024a3-ee13-42ad-ab73-d593488d75e4",
   "metadata": {},
   "source": [
    "### Performance Analysis of Converged Models\n",
    "\n",
    "In this section, the focus shifts to a performance comparison among only the models that successfully converged. By narrowing down the selection, the analysis aims to offer a more accurate evaluation of each architecture's effectiveness.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b50b876-9980-4c2d-a16f-bc6601225e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.473952</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.466808</td>\n",
       "      <td>0.469098</td>\n",
       "      <td>0.472464</td>\n",
       "      <td>0.478738</td>\n",
       "      <td>0.484852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.447704</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>0.436536</td>\n",
       "      <td>0.441444</td>\n",
       "      <td>0.447434</td>\n",
       "      <td>0.453325</td>\n",
       "      <td>0.457688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.441944</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.436744</td>\n",
       "      <td>0.439779</td>\n",
       "      <td>0.442435</td>\n",
       "      <td>0.443921</td>\n",
       "      <td>0.446320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std       min       25%       50%  \\\n",
       "Architecture                                                            \n",
       "Basic          18.0  0.473952  0.005961  0.466808  0.469098  0.472464   \n",
       "GRU            15.0  0.447704  0.007070  0.436536  0.441444  0.447434   \n",
       "LSTM            8.0  0.441944  0.003482  0.436744  0.439779  0.442435   \n",
       "\n",
       "                   75%       max  \n",
       "Architecture                      \n",
       "Basic         0.478738  0.484852  \n",
       "GRU           0.453325  0.457688  \n",
       "LSTM          0.443921  0.446320  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filters the models with a 'Best Loss' less than 0.5 and stores them in 'best_models'.\n",
    "# Then, it displays descriptive statistics of 'Best Loss' for each architecture and visualizes this information using a horizontal boxplot.\n",
    "\n",
    "best_models = final_results[final_results['Best Loss']<0.5]\n",
    "display(best_models.groupby('Architecture').describe()['Best Loss'])\n",
    "\n",
    "# Creates a horizontal boxplot to compare the 'Best Loss' among the different architectures for only the models with 'Best Loss' < 0.5.\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x=\"Best Loss\", y=\"Architecture\", data=best_models, orient='h')\n",
    "plt.title(\"Comparing Converged Models Based on Best Loss\")\n",
    "plt.xlabel(\"Best Loss\")\n",
    "plt.ylabel(\"Architecture\")\n",
    "\n",
    "plt.xlim(best_models['Best Loss'].min()-0.001, best_models['Best Loss'].max()+0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d67e2-f599-48d5-bade-1c2aae641e77",
   "metadata": {},
   "source": [
    "The statistical summary of the 'Best Loss' for converged models highlights different performance aspects across the Basic, GRU, and LSTM architectures. The Basic architecture exhibits a higher mean loss, but with less variability than GRU as indicated by a smaller standard deviation. Meanwhile, GRU models show a slightly higher variability but lower mean loss. The LSTM models stand out for not only having the lowest mean loss but also the least variability, suggesting both effective and consistent performance.\n",
    "\n",
    "To rigorously assess the performance differences among the Basic, GRU, and LSTM architectures, a series of statistical tests were conducted. The aim of these tests is to determine whether the differences in mean loss across these architectures are statistically significant. This step is crucial for confirming whether the observed differences are genuine or could have occurred by chance.\n",
    "\n",
    "Although the statistical tests provide valuable insights into the performance differences among the Basic, GRU, and LSTM architectures, it's important to note the limitations of this analysis. Specifically, the small sample size for the LSTM models—consisting of only 8 converged models—could affect the statistical power and reliability of the test results. Due to this limited sample size, the conclusions drawn about the LSTM architecture should be interpreted with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1f315-8b39-441e-953b-aa433d658ad8",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "858f68f4-2b5b-4ce4-af55-e79d55f188dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro Wilk:\n",
      "GRU: W = 0.9374886751174927, p-value = 0.3518405854701996\n",
      "LSTM: W = 0.9415451288223267, p-value = 0.6263448596000671\n",
      "Basic: W = 0.9110510945320129, p-value = 0.08978526294231415\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Best Loss' data for each neural network architecture (GRU, LSTM, Basic) and performs the Shapiro-Wilk test to assess the normality of the data.\n",
    "# The test results, including the W statistic and p-value, are printed for each architecture.\n",
    "\n",
    "# Filter the 'Best Loss' data for each architecture\n",
    "gru_data = best_models[best_models['Architecture'] == 'GRU']['Best Loss']\n",
    "lstm_data = best_models[best_models['Architecture'] == 'LSTM']['Best Loss']\n",
    "basic_data = best_models[best_models['Architecture'] == 'Basic']['Best Loss']\n",
    "\n",
    "# Perform the Shapiro-Wilk test for normality on each filtered dataset\n",
    "shapiro_test_gru = stats.shapiro(gru_data)\n",
    "shapiro_test_lstm = stats.shapiro(lstm_data)\n",
    "shapiro_test_basic = stats.shapiro(basic_data)\n",
    "\n",
    "# Print the Shapiro-Wilk test results for each architecture\n",
    "print('Shapiro Wilk:')\n",
    "print(f\"GRU: W = {shapiro_test_gru[0]}, p-value = {shapiro_test_gru[1]}\")\n",
    "print(f\"LSTM: W = {shapiro_test_lstm[0]}, p-value = {shapiro_test_lstm[1]}\")\n",
    "print(f\"Basic: W = {shapiro_test_basic[0]}, p-value = {shapiro_test_basic[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38bd35-3e1d-4af3-b9d2-354454623119",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test was conducted to assess the normality of the 'Best Loss' distributions for each architecture. \n",
    "In all cases, the \r\n",
    "p-values are greater than the common alpha level of 0.05, failing to reject the null hypothesis\n",
    ". This suggests that the 'Best Loss' distributions for the GRU, LSTM, and Basic architectures do not significantly deviate from a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac9eda-d15d-4835-85c3-1e97f47e9f23",
   "metadata": {},
   "source": [
    "#### Levene and Bartlett Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b33693d6-8d54-466e-8f7f-038333e2c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levene Test:\n",
      "Statistic: 2.121231406128777, p-value: 0.13386065869571162\n",
      "Bartlett Test:\n",
      "Statistic: 3.665608500057695, p-value: 0.15996435816551732\n"
     ]
    }
   ],
   "source": [
    "# Performs Levene and Bartlett tests to assess the homogeneity of variances across the 'Best Loss' data for each neural network architecture (GRU, LSTM, Basic).\n",
    "# The test results, including the test statistics and p-values, are printed for both tests.\n",
    "\n",
    "# Perform the Levene test for homogeneity of variances\n",
    "levene_test_result = levene(gru_data, lstm_data, basic_data)\n",
    "\n",
    "# Perform the Bartlett test for homogeneity of variances\n",
    "bartlett_test_result = bartlett(gru_data, lstm_data, basic_data)\n",
    "\n",
    "# Print the test results for Levene and Bartlett tests\n",
    "print(\"Levene Test:\")\n",
    "print(f\"Statistic: {levene_test_result.statistic}, p-value: {levene_test_result.pvalue}\")\n",
    "\n",
    "print(\"Bartlett Test:\")\n",
    "print(f\"Statistic: {bartlett_test_result.statistic}, p-value: {bartlett_test_result.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338bbab-12b7-49a5-8e35-f35a6487abb8",
   "metadata": {},
   "source": [
    "Both the Levene and Bartlett tests were conducted to assess the homogeneity of variances across the 'Best Loss' distributions for the different architectures. In both tests, the p-values exceeded the common alpha level of 0.05, indicating a failure to reject the null hypothesis. This suggests that the variances of the 'Best Loss' for the Basic, GRU, and LSTM architectures are not significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10806e-9ada-4f4b-bfbe-f18cfe9d0579",
   "metadata": {},
   "source": [
    "#### ANOVA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34eb3dad-59bc-4c67-a869-84f9073e6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA:\n",
      "F-statistic: 112.63405491012612\n",
      "P-value: 1.0673859551688611e-16\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Best Loss' data for each neural network architecture (GRU, LSTM, Basic) and performs a one-way ANOVA test to compare the means.\n",
    "# The test results, including the F-statistic and p-value, are printed.\n",
    "\n",
    "# Filter the 'Best Loss' data for each architecture\n",
    "gru_data = best_models[best_models['Architecture'] == 'GRU']['Best Loss']\n",
    "lstm_data = best_models[best_models['Architecture'] == 'LSTM']['Best Loss']\n",
    "basic_data = best_models[best_models['Architecture'] == 'Basic']['Best Loss']\n",
    "\n",
    "# Perform the one-way ANOVA test to compare the means\n",
    "f_stat, p_value = stats.f_oneway(gru_data, lstm_data, basic_data)\n",
    "\n",
    "# Print the ANOVA test results\n",
    "print('ANOVA:')\n",
    "print(\"F-statistic:\", f_stat)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fb1f8-4bdc-4312-abab-aef50bf337d0",
   "metadata": {},
   "source": [
    "An ANOVA test was conducted to compare the 'Best Loss' across the Basic, GRU, and LSTM architectures. Given the extremely low \r\n",
    "p-value, far below the common alpha level of 0.05, the null hypothesis is rejected. This suggests that there are statistically significant differences in the 'Best Loss' means among the three architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497aeb8a-85e5-49eb-b54c-94dcde8fe240",
   "metadata": {},
   "source": [
    "#### T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "515f5bb1-aa8e-4446-9bc2-1e9b116be7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Test:\n",
      "T-statistic: 2.152087444425605\n",
      "P-value: 0.043176145833083525\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Best Loss' data for the GRU and LSTM neural network architectures and performs an independent two-sample t-test to compare their means.\n",
    "# The test results, including the T-statistic and p-value, are printed.\n",
    "\n",
    "# Filter the 'Best Loss' data for GRU and LSTM architectures\n",
    "gru_data = best_models[best_models['Architecture'] == 'GRU']['Best Loss']\n",
    "lstm_data = best_models[best_models['Architecture'] == 'LSTM']['Best Loss']\n",
    "\n",
    "# Perform the independent two-sample t-test to compare the means\n",
    "t_stat, p_value = stats.ttest_ind(gru_data, lstm_data)\n",
    "\n",
    "# Print the T-test results\n",
    "print('T-Test:')\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769b1c3-d51c-46e2-b1ed-1dd954311340",
   "metadata": {},
   "source": [
    "A t-test was conducted to evaluate the differences in 'Best Loss' between the GRU and LSTM architectures. Given that the \r\n",
    "p-value is less than the common alpha level of 0.05, the null hypothesis is rejected. This suggests that there is a statistically significant difference in 'Best Loss' between the GRU and LSTM architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2073d4-9bc3-429c-9ddf-466791fc8399",
   "metadata": {},
   "source": [
    "### Training Time Analysis of Converged Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb0c57ef-f0df-4dfe-baab-791143bf652d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1295.206998</td>\n",
       "      <td>380.136244</td>\n",
       "      <td>905.402970</td>\n",
       "      <td>1004.852687</td>\n",
       "      <td>1128.200542</td>\n",
       "      <td>1618.678929</td>\n",
       "      <td>2037.909769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRU</th>\n",
       "      <td>15.0</td>\n",
       "      <td>44732.657092</td>\n",
       "      <td>26487.228280</td>\n",
       "      <td>17000.242563</td>\n",
       "      <td>27042.493517</td>\n",
       "      <td>38000.263731</td>\n",
       "      <td>50410.458306</td>\n",
       "      <td>121803.032488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>8.0</td>\n",
       "      <td>31734.871181</td>\n",
       "      <td>11288.406879</td>\n",
       "      <td>17468.263312</td>\n",
       "      <td>23898.692986</td>\n",
       "      <td>29388.311028</td>\n",
       "      <td>38085.613113</td>\n",
       "      <td>51131.960991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean           std           min           25%  \\\n",
       "Architecture                                                                  \n",
       "Basic          18.0   1295.206998    380.136244    905.402970   1004.852687   \n",
       "GRU            15.0  44732.657092  26487.228280  17000.242563  27042.493517   \n",
       "LSTM            8.0  31734.871181  11288.406879  17468.263312  23898.692986   \n",
       "\n",
       "                       50%           75%            max  \n",
       "Architecture                                             \n",
       "Basic          1128.200542   1618.678929    2037.909769  \n",
       "GRU           38000.263731  50410.458306  121803.032488  \n",
       "LSTM          29388.311028  38085.613113   51131.960991  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filters the models with a 'Best Loss' less than 0.5 and stores them in 'best_models'.\n",
    "# Then, it displays descriptive statistics of 'Duration' for each architecture and visualizes this information using a horizontal boxplot.\n",
    "# The x-axis limits are set to slightly beyond the minimum and maximum 'Duration' values for better visualization.\n",
    "\n",
    "# Filter the models based on 'Best Loss' and display the descriptive statistics for 'Duration'\n",
    "best_models = final_results[final_results['Best Loss']<0.5]\n",
    "display(best_models.groupby('Architecture').describe()['Duration'])\n",
    "\n",
    "# Initialize a new figure for the boxplot with dimensions 20x6.\n",
    "# Create a horizontal boxplot to compare the 'Duration' among the different architectures for only the models with 'Best Loss' < 0.5.\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x=\"Duration\", y=\"Architecture\", data=best_models, orient='h')\n",
    "plt.title(\"Comparando apenas os modelos convergidos\")\n",
    "plt.xlabel(\"Duration (seconds)\")\n",
    "plt.ylabel(\"Architecture\")\n",
    "\n",
    "# Set the x-axis limits to zoom in for better visualization\n",
    "plt.xlim(best_models['Duration'].min()-0.001, best_models['Duration'].max()+0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d431b-c39e-4afd-be40-4957a723d099",
   "metadata": {},
   "source": [
    "While GRU and LSTM architectures may offer better 'Best Loss' metrics, it is evident that they also require substantially more time to train. Specifically, the mean training time for GRU and LSTM models is approximately 44,733 seconds and 31,735 seconds, respectively, compared to just 1,295 seconds for the Basic architecture. This presents an important trade-off to consider, especially when working under time or computational resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7691d1-c5db-462a-9185-228d8fe95fad",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23300d7f-612d-4110-80b7-7dacfdd72dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro Wilk:\n",
      "GRU: W = 0.8288368582725525, p-value = 0.008834054693579674\n",
      "LSTM: W = 0.9412817358970642, p-value = 0.6237343549728394\n",
      "Basic: W = 0.841869056224823, p-value = 0.006320524960756302\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Duration' data for each neural network architecture (GRU, LSTM, Basic) and performs the Shapiro-Wilk test to assess the normality of the data.\n",
    "# The test results, including the W statistic and p-value, are printed for each architecture.\n",
    "\n",
    "# Filter the 'Duration' data for each architecture\n",
    "gru_duration = best_models[best_models['Architecture'] == 'GRU']['Duration']\n",
    "lstm_duration = best_models[best_models['Architecture'] == 'LSTM']['Duration']\n",
    "basic_duration = best_models[best_models['Architecture'] == 'Basic']['Duration']\n",
    "\n",
    "# Perform the Shapiro-Wilk test for normality on each filtered dataset\n",
    "w_gru, p_gru = shapiro(gru_duration)\n",
    "w_lstm, p_lstm = shapiro(lstm_duration)\n",
    "w_basic, p_basic = shapiro(basic_duration)\n",
    "\n",
    "# Print the Shapiro-Wilk test results for each architecture\n",
    "print('Shapiro Wilk:')\n",
    "print(f'GRU: W = {w_gru}, p-value = {p_gru}')\n",
    "print(f'LSTM: W = {w_lstm}, p-value = {p_lstm}')\n",
    "print(f'Basic: W = {w_basic}, p-value = {p_basic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa6b03-6e5b-405a-8e46-a237ae77c5eb",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test was performed to assess the normality of the training time distributions for each architecture. Given that not all groups follow a normal distribution, as indicated by the \r\n",
    "p-values below 0.05 for the GRU and Basic models, non-parametric statistical tests are more appropriate for comparing the group medians in subsequent analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11943c3-97f6-4d28-9885-67f868ee7682",
   "metadata": {},
   "source": [
    "#### Kruskal-Wallis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe644cbd-c212-47c0-82fa-9b3df81f9a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis:\n",
      "H-statistic: 30.05360046457608\n",
      "P-value: 2.9781294993839873e-07\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Duration' data for each neural network architecture (GRU, LSTM, Basic) and performs the Kruskal-Wallis test to compare their median values.\n",
    "# The test results, including the H-statistic and p-value, are printed.\n",
    "\n",
    "# Perform the Kruskal-Wallis test to compare the medians\n",
    "h_stat, p_value = kruskal(gru_duration, lstm_duration, basic_duration)\n",
    "\n",
    "# Print the Kruskal-Wallis test results\n",
    "print('Kruskal-Wallis:')\n",
    "print(\"H-statistic:\", h_stat)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563fbc2-c1f8-45b7-99c2-333a33d8b6b5",
   "metadata": {},
   "source": [
    "The Kruskal-Wallis test was conducted to compare the training time medians across the Basic, GRU, and LSTM architectures. Given the extremely low \r\n",
    "p-value, far below the common alpha level of 0.05, the null hypothesis is rejected. This suggests that there are statistically significant differences in the training time medians among the three architectures. It's worth noting that the Basic models had substantially shorter training times, which could be influencing the overall comparison. Therefore, a separate test was conducted to compare only the GRU and LSTM models, which had more similar training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5debfe-3ab0-47da-a631-90fbb999c61b",
   "metadata": {},
   "source": [
    "#### Mann-Whitney Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1130e120-1896-4ccb-8982-3ff82ddc7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney:\n",
      "U-statistic: 79.0\n",
      "P-value: 0.23820245801669948\n"
     ]
    }
   ],
   "source": [
    "# Filters the 'Duration' data for the GRU and LSTM neural network architectures and performs a two-sided Mann-Whitney U test to compare their median values.\n",
    "# The test results, including the U-statistic and p-value, are printed.\n",
    "\n",
    "# Perform the two-sided Mann-Whitney U test to compare the medians between GRU and LSTM\n",
    "u_stat, p_value = mannwhitneyu(gru_duration, lstm_duration, alternative='two-sided')\n",
    "\n",
    "# Print the Mann-Whitney U test results\n",
    "print('Mann-Whitney:')\n",
    "print(\"U-statistic:\", u_stat)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42250d6-1105-474b-a51e-370259e11d57",
   "metadata": {},
   "source": [
    "A Mann-Whitney test was conducted to specifically compare the training time medians between the GRU and LSTM architectures. Given that the \r\n",
    "p-value is greater than the common alpha level of 0.05, the null hypothesis is not rejected. This suggests that there is no statistically significant difference in the training times between the GRU and LSTM architectures. In other words, based on the obtained data, it cannot be concluded that one architecture is faster or slower to train than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfec6b-c9d2-4bc6-bece-b4acf9c8a976",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118f340-0111-4234-a060-d39d24f050c2",
   "metadata": {},
   "source": [
    "In summary, the comprehensive statistical analysis conducted on the 'Best Loss' and training times of the Basic, GRU, and LSTM architectures provided valuable insights into their performance and efficiency. While the LSTM architecture showed the most promising 'Best Loss' metrics, it was closely followed by the GRU architecture, both of which significantly outperformed the Basic model. However, this superior performance comes at the cost of increased training time, which is an essential factor to consider, particularly in scenarios with limited computational resources.\n",
    "\n",
    "Statistical tests like ANOVA and the t-test corroborated these performance differences by revealing statistically significant variations in 'Best Loss' metrics among the architectures. Moreover, non-parametric tests like Kruskal-Wallis and Mann-Whitney were used due to the non-normal distribution of training times, showing significant differences among all three architectures, but not specifically between GRU and LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
